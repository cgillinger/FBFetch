# fetch_facebook_reach.py

import csv
import json
import os
import time
import requests
import logging
import argparse
import sys
import glob
import pandas as pd
from datetime import datetime, timedelta
from calendar import monthrange
from config import (
    ACCESS_TOKEN, TOKEN_LAST_UPDATED, INITIAL_START_YEAR_MONTH,
    API_VERSION, CACHE_FILE, 
    BATCH_SIZE, MAX_RETRIES, RETRY_DELAY, 
    TOKEN_VALID_DAYS, MAX_REQUESTS_PER_HOUR,
    MONTH_PAUSE_SECONDS
)

# Skapa datumst√§mplad loggfil
def setup_logging():
    """Konfigurera loggning med datumst√§mplad loggfil"""
    now = datetime.now()
    log_dir = "logs"
    
    # Skapa loggdirektory om den inte finns
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    # Skapa datumst√§mplad loggfilnamn
    log_filename = os.path.join(log_dir, f"facebook_reach_{now.strftime('%Y-%m-%d_%H-%M-%S')}.log")
    
    # Konfigurera loggning
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename),  # Datumst√§mplad loggfil
            logging.FileHandler("facebook_reach.log"),  # Beh√•ll den senaste loggfilen f√∂r enkelt √•tkomst
            logging.StreamHandler()  # Terminal-utskrift
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"Startar loggning till fil: {log_filename}")
    
    return logger

# Konfigurera loggning med datumst√§mplad fil
logger = setup_logging()

# R√§knare f√∂r API-anrop
api_call_count = 0
start_time = time.time()

def check_token_expiry():
    """Kontrollera om token snart g√•r ut och varna anv√§ndaren"""
    try:
        last_updated = datetime.strptime(TOKEN_LAST_UPDATED, "%Y-%m-%d")
        days_since = (datetime.now() - last_updated).days
        days_left = TOKEN_VALID_DAYS - days_since
        
        logger.info(f"üîë Token skapades f√∂r {days_since} dagar sedan ({days_left} dagar kvar till utg√•ng).")
        
        if days_left <= 7:
            logger.warning(f"‚ö†Ô∏è VARNING: Din token g√•r ut inom {days_left} dagar! Skapa en ny token snart.")
        elif days_left <= 0:
            logger.error(f"‚ùå KRITISKT: Din token har g√•tt ut! Skapa en ny token omedelbart.")
            sys.exit(1)
    except Exception as e:
        logger.error(f"‚ö†Ô∏è Kunde inte tolka TOKEN_LAST_UPDATED: {e}")

def load_page_cache():
    """Ladda cache med sidnamn f√∂r att minska API-anrop"""
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                logger.debug(f"Laddar sid-cache fr√•n {CACHE_FILE}")
                return json.load(f)
        except json.JSONDecodeError:
            logger.warning(f"Kunde inte ladda cache-fil, skapar ny cache")
    return {}

def save_page_cache(cache):
    """Spara cache med sidnamn f√∂r framtida k√∂rningar"""
    try:
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
            logger.debug(f"Sparade sid-cache till {CACHE_FILE}")
    except Exception as e:
        logger.error(f"Kunde inte spara cache: {e}")

def api_request(url, params, retries=MAX_RETRIES):
    """G√∂r API-f√∂rfr√•gan med √•terf√∂rs√∂k och rate limit-hantering"""
    global api_call_count
    
    # Kontrollera om vi n√§rmar oss rate limit
    current_time = time.time()
    elapsed_hours = (current_time - start_time) / 3600
    rate = api_call_count / elapsed_hours if elapsed_hours > 0 else 0
    
    if rate > MAX_REQUESTS_PER_HOUR * 0.9:  # Om vi anv√§nt 90% av rate limit
        wait_time = 3600 / MAX_REQUESTS_PER_HOUR  # V√§nta tillr√§ckligt f√∂r att h√•lla oss under gr√§nsen
        logger.warning(f"N√§rmar oss rate limit ({int(rate)}/h). V√§ntar {wait_time:.1f} sekunder...")
        time.sleep(wait_time)
    
    for attempt in range(retries):
        try:
            api_call_count += 1
            response = requests.get(url, params=params, timeout=30)
            
            # Hantera vanliga HTTP-fel
            if response.status_code == 429:  # Too Many Requests
                retry_after = int(response.headers.get('Retry-After', RETRY_DELAY))
                logger.warning(f"Rate limit n√•tt! V√§ntar {retry_after} sekunder... (f√∂rs√∂k {attempt+1}/{retries})")
                time.sleep(retry_after)
                continue
                
            elif response.status_code >= 500:  # Server error
                wait_time = RETRY_DELAY * (2 ** attempt)  # Exponentiell backoff
                logger.warning(f"Serverfel: {response.status_code}. V√§ntar {wait_time} sekunder... (f√∂rs√∂k {attempt+1}/{retries})")
                time.sleep(wait_time)
                continue
            
            # F√∂r alla HTTP-svarkoder, f√∂rs√∂k tolka JSON-inneh√•llet
            try:
                json_data = response.json()
                
                # S√§rskild hantering f√∂r 400-fel (Bad Request)
                if response.status_code == 400 and "error" in json_data:
                    error_code = json_data["error"].get("code")
                    error_msg = json_data["error"].get("message", "Ok√§nt fel")
                    
                    # Hantera specifika felkoder
                    if error_code == 4:  # App-specifikt rate limit
                        wait_time = 60 * (attempt + 1)  # V√§nta l√§ngre f√∂r varje f√∂rs√∂k
                        logger.warning(f"App rate limit: {error_msg}. V√§ntar {wait_time} sekunder...")
                        time.sleep(wait_time)
                        continue
                        
                    elif error_code == 190:  # Ogiltig token
                        logger.error(f"Access token ogiltig: {error_msg}")
                        return None
                
                # Om vi kommer hit och har en icke-200 status, logga felet men returnera √§nd√• JSON-data
                # s√• att anropande funktion kan hantera felet mer detaljerat
                if response.status_code != 200:
                    logger.error(f"HTTP-fel {response.status_code}: {response.text}")
                    
                    if attempt < retries - 1:
                        wait_time = RETRY_DELAY * (2 ** attempt)
                        logger.info(f"V√§ntar {wait_time} sekunder innan nytt f√∂rs√∂k... (f√∂rs√∂k {attempt+1}/{retries})")
                        time.sleep(wait_time)
                        continue
                    
                    # Returnera √§nd√• JSON-data s√• att anropande funktion kan hantera felet
                    return json_data
                
                # Allt gick bra, returnera data
                return json_data
                
            except json.JSONDecodeError:
                logger.error(f"Kunde inte tolka JSON-svar: {response.text[:100]}")
                if attempt < retries - 1:
                    wait_time = RETRY_DELAY * (2 ** attempt)
                    logger.info(f"V√§ntar {wait_time} sekunder innan nytt f√∂rs√∂k... (f√∂rs√∂k {attempt+1}/{retries})")
                    time.sleep(wait_time)
                    continue
                return None
                
        except requests.RequestException as e:
            logger.error(f"N√§tverksfel: {e}")
            if attempt < retries - 1:
                wait_time = RETRY_DELAY * (2 ** attempt)
                logger.info(f"V√§ntar {wait_time} sekunder innan nytt f√∂rs√∂k... (f√∂rs√∂k {attempt+1}/{retries})")
                time.sleep(wait_time)
            else:
                return None
    
    return None

def validate_token(token):
    """Validera att token √§r giltig och h√§mta anv√§ndarbeh√∂righeter"""
    logger.info("Validerar token...")
    url = f"https://graph.facebook.com/{API_VERSION}/debug_token"
    params = {"input_token": token, "access_token": token}
    
    data = api_request(url, params)
    
    if not data or "data" not in data:
        logger.error("‚ùå Kunde inte validera token")
        return False
        
    if not data["data"].get("is_valid"):
        logger.error(f"‚ùå Token √§r ogiltig: {data['data'].get('error', {}).get('message', 'Ok√§nd anledning')}")
        return False
        
    logger.info(f"‚úÖ Token validerad. App ID: {data['data'].get('app_id')}")
    return True

def get_page_ids_with_access(token):
    """H√§mta alla sidor som token har √•tkomst till"""
    logger.info("H√§mtar tillg√§ngliga sidor...")
    url = f"https://graph.facebook.com/{API_VERSION}/me/accounts"
    params = {"access_token": token, "limit": 100, "fields": "id,name"}
    
    pages = []
    next_url = url
    
    while next_url:
        data = api_request(url if next_url == url else next_url, {} if next_url != url else params)
        
        if not data or "data" not in data:
            break
            
        pages.extend(data["data"])
        logger.debug(f"Hittade {len(data['data'])} sidor i denna batch")
        
        # Hantera paginering
        next_url = data.get("paging", {}).get("next")
        if next_url and next_url != url:
            logger.debug(f"H√§mtar n√§sta sida fr√•n: {next_url}")
        else:
            break
    
    if not pages:
        logger.warning("Inga sidor hittades. Token kanske saknar 'pages_show_list'-beh√∂righet.")
    
    page_ids = [(page["id"], page["name"]) for page in pages]
    logger.info(f"‚úÖ Hittade {len(page_ids)} sidor att analysera")
    return page_ids

def filter_placeholder_pages(page_list):
    """Filtrera bort placeholder-sidor som SrholderX (d√§r X √§r ett tal)"""
    filtered_pages = []
    filtered_out = []
    
    for page_id, page_name in page_list:
        # Kontrollera om sidnamnet matchar m√∂nstret "SrholderX" d√§r X √§r ett eller flera siffror
        if page_name and page_name.startswith('Srholder') and page_name[8:].isdigit():
            filtered_out.append((page_id, page_name))
            logger.debug(f"Filtrerar bort placeholder-sida: {page_name} (ID: {page_id})")
        else:
            filtered_pages.append((page_id, page_name))
    
    if filtered_out:
        placeholder_names = []
        for _, name in filtered_out:
            placeholder_names.append(name)
        logger.info(f"üö´ Filtrerade bort {len(filtered_out)} placeholder-sidor: {', '.join(placeholder_names)}")
    
    logger.info(f"‚úÖ {len(filtered_pages)} sidor kvar efter filtrering")
    return filtered_pages

def get_page_name(page_id, cache):
    """H√§mta sidans namn fr√•n cache eller API"""
    if page_id in cache:
        return cache[page_id]
    
    logger.debug(f"H√§mtar namn f√∂r sida {page_id}...")
    url = f"https://graph.facebook.com/{API_VERSION}/{page_id}"
    params = {"fields": "name", "access_token": ACCESS_TOKEN}
    
    data = api_request(url, params)
    
    if not data or "error" in data:
        error_msg = data.get("error", {}).get("message", "Ok√§nt fel") if data else "Fel vid API-anrop"
        logger.warning(f"‚ö†Ô∏è Kunde inte h√§mta namn f√∂r sida {page_id}: {error_msg}")
        return None
    
    name = data.get("name", f"Page {page_id}")
    cache[page_id] = name
    return name

def get_page_access_token(page_id, system_token):
    """Konvertera systemanv√§ndartoken till en Page Access Token f√∂r en specifik sida"""
    logger.debug(f"H√§mtar Page Access Token f√∂r sida {page_id}...")
    url = f"https://graph.facebook.com/{API_VERSION}/{page_id}"
    params = {
        "fields": "access_token",
        "access_token": system_token
    }
    
    data = api_request(url, params)
    
    if not data or "error" in data or "access_token" not in data:
        error_msg = data.get("error", {}).get("message", "Ok√§nt fel") if data and "error" in data else "Kunde inte h√§mta token"
        logger.warning(f"‚ö†Ô∏è Kunde inte h√§mta Page Access Token f√∂r sida {page_id}: {error_msg}")
        return None
    
    return data["access_token"]

def get_page_publications(page_id, page_token, since, until, page_name=None):
    """H√§mta antal publiceringar f√∂r en sida under en specifik tidsperiod"""
    display_name = page_name if page_name else page_id
    logger.debug(f"H√§mtar antal publiceringar f√∂r sida {display_name} fr√•n {since} till {until}...")
    
    try:
        # Konvertera datum till Unix timestamps som Facebook API f√∂redrar
        since_timestamp = int(datetime.strptime(since, "%Y-%m-%d").timestamp())
        until_timestamp = int(datetime.strptime(until, "%Y-%m-%d").timestamp()) + 86399  # L√§gg till 23:59:59
        
        url = f"https://graph.facebook.com/{API_VERSION}/{page_id}/published_posts"
        params = {
            "access_token": page_token,
            "since": since_timestamp,
            "until": until_timestamp,
            "summary": "total_count"
        }
        
        logger.debug(f"Publications API-anrop f√∂r {display_name}: {url}")
        logger.debug(f"Parameters: since={since_timestamp} ({since}), until={until_timestamp} ({until})")
        
        data = api_request(url, params)
        
        logger.debug(f"Publications API-svar f√∂r {display_name}: {data}")
        
        if data and "summary" in data and "total_count" in data["summary"]:
            publications_count = data["summary"]["total_count"]
            logger.info(f"  ‚úì Publiceringar f√∂r {display_name}: {publications_count} (fr√•n {since} till {until})")
            return publications_count
        elif data and "error" in data:
            error_msg = data["error"].get("message", "Ok√§nt fel")
            error_code = data["error"].get("code", "N/A")
            logger.error(f"Error {error_code}: Kunde inte h√§mta publiceringar f√∂r sida '{display_name}': {error_msg}")
            
            # Prova utan datumfilter som fallback
            logger.info(f"F√∂rs√∂ker h√§mta totalt antal publiceringar f√∂r {display_name} utan datumfilter...")
            fallback_params = {
                "access_token": page_token,
                "summary": "total_count"
            }
            
            fallback_data = api_request(url, fallback_params)
            if fallback_data and "summary" in fallback_data:
                total_posts = fallback_data["summary"]["total_count"]
                logger.warning(f"  ‚ö†Ô∏è Fallback: Totalt antal posts f√∂r {display_name}: {total_posts} (alla tider)")
                logger.warning(f"  ‚ö†Ô∏è Anv√§nder 0 f√∂r denna period eftersom datumfilter misslyckades")
            
            return 0
        else:
            logger.warning(f"  ‚úó Kunde inte h√§mta publiceringar f√∂r sida {display_name}: Inget data")
            logger.debug(f"Ov√§ntat API-svar: {data}")
            return 0
            
    except Exception as e:
        logger.error(f"  ‚úó Fel vid h√§mtning av publiceringar f√∂r sida {display_name}: {e}")
        return 0

def get_page_metrics(page_id, system_token, since, until, page_name=None):
    """H√§mta r√§ckvidd, interaktionsdata och antal publiceringar f√∂r en sida under en specifik tidsperiod"""
    display_name = page_name if page_name else page_id
    logger.debug(f"H√§mtar metriker f√∂r sida {display_name} fr√•n {since} till {until}...")
    
    # Skapa resultatstruktur
    result = {
        "reach": 0,
        "engaged_users": 0,
        "engagements": 0,
        "reactions": 0,
        "clicks": 0,
        "publications": 0,      # Ny: antal publiceringar
        "reactions_details": {},  # Lagra detaljerade reaktionsdata
        "status": "OK",           # Defaultstatus
        "comment": ""             # Plats f√∂r ytterligare information om felet
    }
    
    # F√∂rst h√§mta en Page Access Token f√∂r denna specifika sida
    page_token = get_page_access_token(page_id, system_token)
    
    if not page_token:
        result["status"] = "NO_ACCESS"
        result["comment"] = "Kunde inte h√§mta Page Access Token"
        logger.warning(f"‚ö†Ô∏è Kunde inte h√§mta Page Access Token f√∂r sida {display_name}")
        return result
    
    # H√§mta antal publiceringar f√∂rst (anv√§nder Page Access Token)
    result["publications"] = get_page_publications(page_id, page_token, since, until, page_name)
    
    # Definition av metriker och deras mappning
    metrics_mapping = [
        {"api_name": "page_impressions_unique", "result_key": "reach", "display_name": "R√§ckvidd"},
        {"api_name": "page_post_engagements", "result_key": "engagements", "display_name": "Interaktioner"},
        {"api_name": "page_actions_post_reactions_total", "result_key": "reactions", "display_name": "Reaktioner"},
        {"api_name": "page_consumptions", "result_key": "clicks", "display_name": "Klick"}
    ]
    
    api_errors = []  # Samla fel fr√•n API-anrop
    
    # H√§mta varje metrik separat f√∂r att isolera fel
    for metric_info in metrics_mapping:
        try:
            # Anv√§nd Page Access Token f√∂r att h√§mta insikter
            url = f"https://graph.facebook.com/{API_VERSION}/{page_id}/insights"
            params = {
                "access_token": page_token,
                "since": since,
                "until": until,
                "period": "total_over_range",
                "metric": metric_info["api_name"]
            }
            
            data = api_request(url, params)
            
            if data and "data" in data and data["data"]:
                # Extrahera v√§rden fr√•n svaret
                for metric in data["data"]:
                    if metric["values"] and len(metric["values"]) > 0:
                        value = metric["values"][0].get("value", 0)
                        
                        # S√§rskild hantering f√∂r reaktioner som kan vara dictionary
                        if metric_info["result_key"] == "reactions" and isinstance(value, dict):
                            # Spara detaljerade reaktionsdata
                            result["reactions_details"] = value
                            # Ber√§kna summan av alla reaktioner
                            total_reactions = sum(int(v) for k, v in value.items() 
                                              if isinstance(v, (int, float)) or 
                                              (isinstance(v, str) and v.isdigit()))
                            
                            logger.info(f"Reaktioner f√∂r {display_name}: {value}, totalt: {total_reactions}")
                            result[metric_info["result_key"]] = total_reactions
                        else:
                            result[metric_info["result_key"]] = value
                            
                        logger.debug(f"  ‚úì {metric_info['display_name']} f√∂r {display_name}: {value}")
            elif data and "error" in data:
                # H√§r f√•ngar vi upp och ger ett tydligt felmeddelande per metrik
                error_msg = data["error"].get("message", "Ok√§nt fel")
                error_code = data["error"].get("code", "N/A")
                api_errors.append(f"{metric_info['display_name']}: {error_msg} (kod {error_code})")
                logger.error(f"Error {error_code}: Saknas m√§tv√§rde '{metric_info['display_name']}' f√∂r sida '{display_name}': {error_msg}")
            else:
                logger.warning(f"  ‚úó Kunde inte h√§mta {metric_info['display_name']} f√∂r sida {display_name}: Inget data")
                
        except Exception as e:
            # Logga felet f√∂r denna specifika metrik
            api_errors.append(f"{metric_info['display_name']}: {str(e)}")
            logger.warning(f"  ‚úó Fel vid h√§mtning av {metric_info['display_name']} f√∂r sida {display_name}: {e}")
            continue
    
    # Kontrollera och uppdatera status baserat p√• resultatet
    if api_errors:
        result["status"] = "API_ERROR"
        result["comment"] = "; ".join(api_errors[:3])  # Begr√§nsa l√§ngden p√• kommentaren
    elif all(result[key] == 0 for key in ["reach", "engaged_users", "engagements", "reactions", "clicks", "publications"]):
        result["status"] = "NO_DATA"
        result["comment"] = "Alla v√§rden √§r noll"
    
    # Returnera resultatet oavsett status
    return result

def read_existing_csv(filename):
    """L√§s in befintlig CSV-fil och returnera en dict med Page ID -> data och info om saknade kolumner"""
    existing_data = {}
    missing_columns = set()
    
    if os.path.exists(filename):
        try:
            with open(filename, mode="r", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                fieldnames = reader.fieldnames or []
                
                # Definiera alla f√∂rv√§ntade kolumner
                expected_columns = {
                    "Page", "Page ID", "Reach", "Engaged Users", "Engagements", 
                    "Reactions", "Clicks", "Publications", "Status", "Comment"
                }
                
                # Identifiera saknade kolumner
                missing_columns = expected_columns - set(fieldnames)
                
                if missing_columns:
                    logger.info(f"Saknade kolumner i {filename}: {', '.join(missing_columns)}")
                
                for row in reader:
                    if "Page ID" in row:
                        # Konvertera numeriska v√§rden till heltal
                        page_data = {
                            "Page": row["Page"],
                            "Page ID": row["Page ID"],
                            "Reach": int(row.get("Reach", 0))
                        }
                        
                        # Hantera nya interaktionsf√§lt om de finns
                        if "Engaged Users" in row:
                            page_data["Engaged Users"] = int(row.get("Engaged Users", 0))
                        if "Engagements" in row:
                            page_data["Engagements"] = int(row.get("Engagements", 0))
                        if "Reactions" in row:
                            # Konvertera Reactions till heltal om m√∂jligt
                            try:
                                page_data["Reactions"] = int(row.get("Reactions", 0))
                            except ValueError:
                                # Om det √§r ett dictionary eller annat format som inte kan konverteras
                                page_data["Reactions"] = 0
                        if "Clicks" in row:
                            page_data["Clicks"] = int(row.get("Clicks", 0))
                        
                        # Hantera Publications (ny kolumn)
                        if "Publications" in row:
                            page_data["Publications"] = int(row.get("Publications", 0))
                        
                        # Hantera statusf√§lt om det finns
                        if "Status" in row:
                            page_data["Status"] = row["Status"]
                        if "Comment" in row:
                            page_data["Comment"] = row["Comment"]
                            
                        existing_data[row["Page ID"]] = page_data
            logger.info(f"L√§ste in {len(existing_data)} befintliga sidor fr√•n {filename}")
        except Exception as e:
            logger.error(f"Fel vid inl√§sning av befintlig CSV-fil {filename}: {e}")
    
    return existing_data, missing_columns

def get_missing_data_for_page(page_id, page_token, since, until, missing_columns, page_name=None):
    """H√§mta endast saknade kolumner f√∂r en befintlig sida"""
    display_name = page_name if page_name else page_id
    logger.debug(f"H√§mtar saknade data f√∂r sida {display_name}: {', '.join(missing_columns)}")
    
    result = {}
    
    # H√§mta Publications om den saknas
    if "Publications" in missing_columns:
        result["Publications"] = get_page_publications(page_id, page_token, since, until, page_name)
    
    # H√§r kan fler kolumner l√§ggas till i framtiden om n√∂dv√§ndigt
    # Till exempel om vi l√§gger till fler metriker senare
    
    return result

def update_existing_page_data(existing_data, page_id, missing_data):
    """Uppdatera befintlig siddata med saknade v√§rden"""
    if page_id in existing_data:
        for key, value in missing_data.items():
            existing_data[page_id][key] = value
        
        # Uppdatera status f√∂r att visa att sidan har uppdaterats
        existing_data[page_id]["Status"] = "UPDATED"
        existing_data[page_id]["Comment"] = "Saknade kolumner tillagda"

def process_in_batches(page_list, cache, start_date, end_date, existing_data=None, missing_columns=None, batch_size=BATCH_SIZE):
    """Bearbeta sidor i batches f√∂r att f√∂rb√§ttra prestanda"""
    total_pages = len(page_list)
    results = []
    success = 0
    failed = 0
    skipped = 0
    updated = 0
    
    # Om vi har befintlig data, l√§gg till den i resultatlistan f√∂rst
    if existing_data:
        results = list(existing_data.values())
        
    # Skapa en upps√§ttning av sidor som redan finns i befintlig data
    existing_page_ids = set(existing_data.keys()) if existing_data else set()
    
    # Best√§m vilka sidor som beh√∂ver bearbetas
    pages_needing_full_processing = []
    pages_needing_partial_update = []
    
    for page_id, page_name in page_list:
        if page_id in existing_page_ids:
            # Sida finns redan - kontrollera om den beh√∂ver uppdateras med saknade kolumner
            if missing_columns and missing_columns:
                pages_needing_partial_update.append((page_id, page_name))
            else:
                # Inga saknade kolumner, hoppa √∂ver
                pass
        else:
            # Ny sida som beh√∂ver all data
            pages_needing_full_processing.append((page_id, page_name))
    
    # Logga vad som kommer att g√∂ras
    logger.info(f"üìä Bearbetningsplan:")
    logger.info(f"  - Nya sidor (full bearbetning): {len(pages_needing_full_processing)}")
    logger.info(f"  - Befintliga sidor (partiell uppdatering): {len(pages_needing_partial_update)}")
    logger.info(f"  - Hoppar √∂ver: {len(existing_page_ids) - len(pages_needing_partial_update)}")
    
    # Bearbeta nya sidor (full bearbetning)
    if pages_needing_full_processing:
        logger.info(f"üÜï Bearbetar {len(pages_needing_full_processing)} nya sidor...")
        batch_start = 0
        for i in range(0, len(pages_needing_full_processing), batch_size):
            batch = pages_needing_full_processing[i:i+batch_size]
            logger.info(f"Bearbetar ny-sidor batch {i//batch_size + 1}/{(len(pages_needing_full_processing) + batch_size - 1)//batch_size} ({len(batch)} sidor)")
            
            for page_id, page_name in batch:
                try:
                    name = page_name or get_page_name(page_id, cache)
                    
                    if not name:
                        logger.warning(f"‚ö†Ô∏è Kunde inte hitta namn f√∂r sida {page_id}, hoppar √∂ver")
                        failed += 1
                        continue
                    
                    logger.info(f"üìä H√§mtar FULL data f√∂r: {name} (ID: {page_id})")
                    metrics = get_page_metrics(page_id, ACCESS_TOKEN, start_date, end_date, page_name=name)
                    
                    if metrics is not None:
                        page_result = {
                            "Page": name,
                            "Page ID": page_id,
                            "Reach": metrics["reach"],
                            "Engaged Users": metrics["engaged_users"],
                            "Engagements": metrics["engagements"],
                            "Reactions": metrics["reactions"],
                            "Clicks": metrics["clicks"],
                            "Publications": metrics["publications"],
                            "Status": metrics["status"],
                            "Comment": metrics.get("comment", "")
                        }
                        
                        results.append(page_result)
                        success += 1
                    else:
                        logger.warning(f"‚ö†Ô∏è Inga data f√∂r sida {page_id} ({name})")
                        results.append({
                            "Page": name,
                            "Page ID": page_id,
                            "Reach": 0,
                            "Engaged Users": 0,
                            "Engagements": 0,
                            "Reactions": 0,
                            "Clicks": 0,
                            "Publications": 0,
                            "Status": "UNKNOWN",
                            "Comment": "Ov√§ntat fel vid h√§mtning av data"
                        })
                        failed += 1
                except Exception as e:
                    logger.error(f"Fel vid bearbetning av sida {page_id}: {e}")
                    failed += 1
    
    # Bearbeta befintliga sidor (partiell uppdatering)
    if pages_needing_partial_update:
        logger.info(f"üîÑ Uppdaterar {len(pages_needing_partial_update)} befintliga sidor med saknade kolumner...")
        for i in range(0, len(pages_needing_partial_update), batch_size):
            batch = pages_needing_partial_update[i:i+batch_size]
            logger.info(f"Bearbetar uppdatering-batch {i//batch_size + 1}/{(len(pages_needing_partial_update) + batch_size - 1)//batch_size} ({len(batch)} sidor)")
            
            for page_id, page_name in batch:
                try:
                    name = page_name or get_page_name(page_id, cache)
                    
                    if not name:
                        logger.warning(f"‚ö†Ô∏è Kunde inte hitta namn f√∂r sida {page_id}, hoppar √∂ver")
                        failed += 1
                        continue
                    
                    logger.info(f"üîÑ Uppdaterar saknade data f√∂r: {name} (ID: {page_id})")
                    
                    # H√§mta Page Access Token
                    page_token = get_page_access_token(page_id, ACCESS_TOKEN)
                    
                    if not page_token:
                        logger.warning(f"‚ö†Ô∏è Kunde inte h√§mta Page Access Token f√∂r sida {name}")
                        failed += 1
                        continue
                    
                    # H√§mta endast saknade data
                    missing_data = get_missing_data_for_page(page_id, page_token, start_date, end_date, missing_columns, name)
                    
                    # Uppdatera befintlig data
                    update_existing_page_data(existing_data, page_id, missing_data)
                    updated += 1
                    
                except Exception as e:
                    logger.error(f"Fel vid uppdatering av sida {page_id}: {e}")
                    failed += 1
    
    # R√§kna skippade (sidor som inte beh√∂vde n√•gon uppdatering)
    skipped = len(existing_page_ids) - len(pages_needing_partial_update)
    
    # Visa framsteg
    total_processed = success + failed + skipped + updated
    progress = total_processed / (total_pages + len(existing_page_ids)) * 100 if (total_pages + len(existing_page_ids)) > 0 else 0
    logger.info(f"‚úÖ Slutresultat: {success} nya, {updated} uppdaterade, {skipped} skippade, {failed} misslyckade")
    
    # Spara cache regelbundet f√∂r att inte f√∂rlora data vid fel
    save_page_cache(cache)
    
    return results, success, failed, skipped, updated

def safe_int_value(value, default=0):
    """S√§kerst√§ller att ett v√§rde √§r ett heltal, och hanterar olika datatyper"""
    if isinstance(value, (int, float)):
        return int(value)
    elif isinstance(value, str) and value.strip().isdigit():
        return int(value)
    elif isinstance(value, dict):
        # Om det √§r ett dictionary med reaktioner, summera alla v√§rden
        try:
            # Filtrera ut eventuella icke-numeriska v√§rden
            total = sum(int(v) for k, v in value.items() if isinstance(v, (int, float)) or (isinstance(v, str) and v.isdigit()))
            logger.info(f"Summerar reaktioner fr√•n dictionary: {value} = {total}")
            return total
        except Exception as e:
            logger.warning(f"Kunde inte summera dictionary-v√§rde: {value}, fel: {e}, anv√§nder 0")
            return default
    else:
        return default

def save_results(data, filename):
    """Spara resultaten till en CSV-fil"""
    try:
        # Sortera resultaten efter r√§ckvidd (h√∂gst f√∂rst)
        sorted_data = sorted(data, key=lambda x: safe_int_value(x.get("Reach", 0)), reverse=True)
        
        # Definiera f√§ltnamn baserat p√• tillg√§ngliga nycklar i f√∂rsta raden
        fieldnames = ["Page", "Page ID", "Reach"]
        
        # L√§gg till interaktionsf√§lt om de finns
        if sorted_data and len(sorted_data) > 0:
            if "Engaged Users" in sorted_data[0]:
                fieldnames.append("Engaged Users")
            if "Engagements" in sorted_data[0]:
                fieldnames.append("Engagements")
            if "Reactions" in sorted_data[0]:
                fieldnames.append("Reactions")
            if "Clicks" in sorted_data[0]:
                fieldnames.append("Clicks")
            if "Publications" in sorted_data[0]:  # Ny kolumn
                fieldnames.append("Publications")
            # L√§gg till Status och Comment om de finns
            if "Status" in sorted_data[0]:
                fieldnames.append("Status")
            if "Comment" in sorted_data[0]:
                fieldnames.append("Comment")
        
        with open(filename, mode="w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(sorted_data)
            
        logger.info(f"‚úÖ Sparade data till {filename}")
        return True
    except Exception as e:
        logger.error(f"‚ùå Kunde inte spara data: {e}")
        return False

def get_existing_reports():
    """Scanna katalogen efter befintliga Facebook-r√§ckviddsrapporter och returnera en upps√§ttning av bearbetade m√•nader (YYYY-MM)"""
    existing_reports = set()
    for filename in glob.glob("FB_*.csv"):
        try:
            # Extrahera √•r och m√•nad fr√•n filnamnet (FB_YYYY_MM.csv)
            parts = filename.replace(".csv", "").split("_")
            if len(parts) == 3 and parts[0] == "FB":
                year = parts[1]
                month = parts[2]
                if year.isdigit() and month.isdigit() and len(year) == 4 and len(month) == 2:
                    existing_reports.add(f"{year}-{month}")
                    logger.debug(f"Hittade befintlig rapport f√∂r {year}-{month}: {filename}")
        except Exception as e:
            logger.warning(f"Kunde inte tolka filnamn {filename}: {e}")
    return existing_reports

def get_missing_months(existing_reports, start_year_month):
    """Best√§m vilka m√•nader som beh√∂ver bearbetas"""
    missing_months = []
    
    # Tolka start√•r och m√•nad
    start_year, start_month = map(int, start_year_month.split("-"))
    
    # H√§mta aktuellt √•r och m√•nad
    now = datetime.now()
    current_year = now.year
    current_month = now.month
    
    # Generera alla m√•nader fr√•n startdatum till sista avslutade m√•nad
    year = start_year
    month = start_month
    
    while (year < current_year) or (year == current_year and month < current_month):
        month_str = f"{year}-{month:02d}"
        if month_str not in existing_reports:
            missing_months.append((year, month))
        
        # G√• till n√§sta m√•nad
        month += 1
        if month > 12:
            month = 1
            year += 1
    
    return missing_months

def analyze_page_presence(previous_month, current_month):
    """
    J√§mf√∂r sidor mellan tv√• m√•nader och identifierar nya och bortfallna sidor.
    
    Args:
        previous_month: S√∂kv√§g till CSV-fil f√∂r f√∂reg√•ende m√•nad eller pandas DataFrame
        current_month: S√∂kv√§g till CSV-fil f√∂r aktuell m√•nad eller pandas DataFrame
        
    Returns:
        DataFrame med: Page ID, Page, Status (NY, BORTFALLEN, OF√ñR√ÑNDRAD), M√•nad
    """
    logger.info(f"Analyserar sidn√§rvaro mellan {previous_month} och {current_month}")
    
    # Konvertera till DataFrame om str√§ngar ges
    prev_df = pd.read_csv(previous_month) if isinstance(previous_month, str) else previous_month
    curr_df = pd.read_csv(current_month) if isinstance(current_month, str) else current_month
    
    # Extrahera √•r och m√•nad fr√•n filnamn om det √§r en str√§ng
    if isinstance(current_month, str):
        parts = current_month.replace(".csv", "").split("_")
        if len(parts) >= 3:
            month_str = f"{parts[1]}-{parts[2]}"
        else:
            month_str = "Ok√§nd"
    else:
        month_str = "Aktuell"
    
    # Hitta nya sidor (i current_month men inte i previous_month)
    prev_page_ids = set(prev_df["Page ID"].astype(str))
    curr_page_ids = set(curr_df["Page ID"].astype(str))
    
    new_page_ids = curr_page_ids - prev_page_ids
    missing_page_ids = prev_page_ids - curr_page_ids
    unchanged_page_ids = prev_page_ids.intersection(curr_page_ids)
    
    # Skapa en lista med alla sidor och deras status
    results = []
    
    # L√§gg till nya sidor
    for page_id in new_page_ids:
        page_info = curr_df[curr_df["Page ID"].astype(str) == page_id].iloc[0]
        results.append({
            "Page ID": page_id,
            "Page": page_info["Page"],
            "Status": "NY",
            "M√•nad": month_str,
            "Kommentar": "Inte med i f√∂reg√•ende m√•nad"
        })
    
    # L√§gg till bortfallna sidor
    for page_id in missing_page_ids:
        page_info = prev_df[prev_df["Page ID"].astype(str) == page_id].iloc[0]
        results.append({
            "Page ID": page_id,
            "Page": page_info["Page"],
            "Status": "BORTFALLEN",
            "M√•nad": month_str,
            "Kommentar": "Fanns i f√∂reg√•ende m√•nad"
        })
    
    # L√§gg till statusuppdateringar f√∂r nuvarande m√•nad
    for _, row in curr_df.iterrows():
        page_id = str(row["Page ID"])
        if "Status" in row and row["Status"] != "OK" and row["Status"] != "SKIPPED":
            results.append({
                "Page ID": page_id,
                "Page": row["Page"],
                "Status": row["Status"],
                "M√•nad": month_str,
                "Kommentar": row.get("Comment", "")
            })
    
    # Konvertera till DataFrame och returnera
    result_df = pd.DataFrame(results)
    
    logger.info(f"Analys klar: {len(new_page_ids)} nya sidor, {len(missing_page_ids)} bortfallna sidor")
    
    return result_df

def save_status_report(status_df, year, month):
    """Sparar en statusrapport f√∂r en specifik m√•nad"""
    filename = f"FB_STATUS_{year}_{month:02d}.csv"
    
    try:
        status_df.to_csv(filename, index=False, encoding="utf-8")
        logger.info(f"‚úÖ Sparade statusrapport till {filename}")
        return True
    except Exception as e:
        logger.error(f"‚ùå Kunde inte spara statusrapport: {e}")
        return False

def generate_custom_filename(start_date, end_date):
    """Generera filnamn f√∂r custom datumintervall"""
    start_obj = datetime.strptime(start_date, "%Y-%m-%d")
    end_obj = datetime.strptime(end_date, "%Y-%m-%d")
    
    # Om start och slut √§r inom samma m√•nad
    if start_obj.month == end_obj.month and start_obj.year == end_obj.year:
        if start_obj.day == 1 and end_obj == datetime(end_obj.year, end_obj.month, monthrange(end_obj.year, end_obj.month)[1]):
            # Hel m√•nad
            return f"FB_{start_obj.year}_{start_obj.month:02d}.csv"
        else:
            # Partiell m√•nad
            return f"FB_{start_obj.year}_{start_obj.month:02d}_{start_obj.day:02d}-{end_obj.day:02d}.csv"
    else:
        # √ñver m√•nader eller √•r
        return f"FB_{start_obj.strftime('%Y-%m-%d')}_to_{end_obj.strftime('%Y-%m-%d')}.csv"

def parse_date_args(args):
    """Tolka kommandoradsargument f√∂r datumintervall och returnera (start_date, end_date)"""
    today = datetime.now().date()
    
    # Custom datum fr√•n argumenten
    if args.from_date and args.to_date:
        try:
            start_date = datetime.strptime(args.from_date, "%Y-%m-%d").date()
            end_date = datetime.strptime(args.to_date, "%Y-%m-%d").date()
            return str(start_date), str(end_date)
        except ValueError:
            logger.error("Felaktigt datumformat. Anv√§nd YYYY-MM-DD")
            sys.exit(1)
    
    # Nuvarande m√•nad hittills
    if args.current_month_so_far:
        start_date = today.replace(day=1)  # F√∂rsta dagen i m√•naden
        end_date = today
        return str(start_date), str(end_date)
    
    # Senaste N dagar
    if args.last_n_days:
        try:
            days = int(args.last_n_days)
            start_date = today - timedelta(days=days-1)  # -1 eftersom vi inkluderar idag
            end_date = today
            return str(start_date), str(end_date)
        except ValueError:
            logger.error("--last-n-days m√•ste vara ett heltal")
            sys.exit(1)
    
    # Senaste veckan
    if args.last_week:
        start_date = today - timedelta(days=6)  # Inkluderar idag
        end_date = today
        return str(start_date), str(end_date)
    
    # Senaste m√•naden (30 dagar)
    if args.last_month:
        start_date = today - timedelta(days=29)  # Inkluderar idag
        end_date = today
        return str(start_date), str(end_date)
    
    return None, None

def process_custom_period(start_date, end_date, cache, page_list=None, update_all=False):
    """Bearbeta data f√∂r ett custom datumintervall"""
    logger.info(f"Bearbetar custom period: {start_date} till {end_date}")
    
    # Generera filnamn f√∂r custom period
    output_file = generate_custom_filename(start_date, end_date)
    
    # H√§mta sidlista om den inte redan h√§mtats
    if not page_list:
        page_list = get_page_ids_with_access(ACCESS_TOKEN)
    
    if not page_list:
        logger.error("‚ùå Inga sidor hittades. Avbryter.")
        return False
    
    # Filtrera bort placeholder-sidor
    page_list = filter_placeholder_pages(page_list)
    
    if not page_list:
        logger.error("‚ùå Inga sidor kvar efter filtrering. Avbryter.")
        return False
    
    # Kontrollera om det finns befintlig data f√∂r denna period
    existing_data = {}
    missing_columns = set()
    if os.path.exists(output_file) and not update_all:
        existing_data, missing_columns = read_existing_csv(output_file)
        logger.info(f"Hittade {len(existing_data)} befintliga sidor i fil {output_file}")
        if missing_columns:
            logger.info(f"Saknade kolumner kommer att l√§ggas till: {', '.join(missing_columns)}")
    
    # Bearbeta data f√∂r denna period
    all_data, ok, fail, skipped, updated = process_in_batches(
        page_list, cache, start_date, end_date, 
        existing_data=None if update_all else existing_data,
        missing_columns=None if update_all else missing_columns
    )
    
    # Spara resultaten
    if all_data:
        save_results(all_data, output_file)
        
        # Visa total r√§ckvidd och interaktioner
        try:
            total_reach = sum(safe_int_value(item.get("Reach", 0)) for item in all_data)
            
            # Ber√§kna totaler f√∂r interaktioner om tillg√§ngligt
            has_engaged = any("Engaged Users" in item for item in all_data)
            has_engagements = any("Engagements" in item for item in all_data)
            has_reactions = any("Reactions" in item for item in all_data)
            has_clicks = any("Clicks" in item for item in all_data)
            has_publications = any("Publications" in item for item in all_data)  # Ny
            
            if has_engaged:
                total_engaged = sum(safe_int_value(item.get("Engaged Users", 0)) for item in all_data)
            else:
                total_engaged = 0
                
            if has_engagements:
                total_engagements = sum(safe_int_value(item.get("Engagements", 0)) for item in all_data)
            else:
                total_engagements = 0
                
            if has_reactions:
                total_reactions = sum(safe_int_value(item.get("Reactions", 0)) for item in all_data)
            else:
                total_reactions = 0
                
            if has_clicks:
                total_clicks = sum(safe_int_value(item.get("Clicks", 0)) for item in all_data)
            else:
                total_clicks = 0
                
            if has_publications:
                total_publications = sum(safe_int_value(item.get("Publications", 0)) for item in all_data)
            else:
                total_publications = 0
            
            logger.info(f"üìà Summering f√∂r {start_date} till {end_date}:")
            logger.info(f"  - Total r√§ckvidd: {total_reach:,}")
            
            if has_engaged:
                logger.info(f"  - Engagerade anv√§ndare: {total_engaged:,}")
            if has_engagements:
                logger.info(f"  - Totala interaktioner: {total_engagements:,}")
            if has_reactions:
                logger.info(f"  - Reaktioner: {total_reactions:,}")
            if has_clicks:
                logger.info(f"  - Klick: {total_clicks:,}")
            if has_publications:
                logger.info(f"  - Publiceringar: {total_publications:,}")  # Ny
            
            if skipped > 0:
                logger.info(f"üìà {skipped} sidor hoppades √∂ver")
            if updated > 0:
                logger.info(f"üîÑ {updated} sidor uppdaterades med saknade kolumner")
                
            # Statusrapport
            status_counts = {}
            for item in all_data:
                if "Status" in item:
                    status = item["Status"]
                    status_counts[status] = status_counts.get(status, 0) + 1
            
            if status_counts:
                logger.info(f"üìã Status√∂versikt:")
                for status, count in status_counts.items():
                    logger.info(f"  - {status}: {count} sidor")
        
        except Exception as e:
            logger.error(f"Fel vid ber√§kning av summor: {e}")
        
        return True
    else:
        logger.warning(f"‚ö†Ô∏è Inga data att spara f√∂r {start_date} till {end_date}")
        return False

def process_month(year, month, cache, page_list=None, update_all=False, generate_status=True):
    """Bearbeta data f√∂r en specifik m√•nad"""
    # S√§tt datumintervall f√∂r m√•naden
    start_date = f"{year}-{month:02d}-01"
    
    # Ber√§kna slutdatum (sista dagen i m√•naden)
    last_day = monthrange(year, month)[1]
    end_date = f"{year}-{month:02d}-{last_day}"
    
    # S√§tt utdatafilnamn
    output_file = f"FB_{year}_{month:02d}.csv"
    
    logger.info(f"Bearbetar m√•nad: {year}-{month:02d} (fr√•n {start_date} till {end_date})")
    
    # H√§mta sidlista om den inte redan h√§mtats
    if not page_list:
        page_list = get_page_ids_with_access(ACCESS_TOKEN)
    
    if not page_list:
        logger.error("‚ùå Inga sidor hittades. Avbryter.")
        return False
    
    # Filtrera bort placeholder-sidor
    page_list = filter_placeholder_pages(page_list)
    
    if not page_list:
        logger.error("‚ùå Inga sidor kvar efter filtrering. Avbryter.")
        return False
    
    # Kontrollera om det finns befintlig data f√∂r denna m√•nad
    existing_data = {}
    missing_columns = set()
    if os.path.exists(output_file) and not update_all:
        existing_data, missing_columns = read_existing_csv(output_file)
        logger.info(f"Hittade {len(existing_data)} befintliga sidor i fil {output_file}")
        if missing_columns:
            logger.info(f"Saknade kolumner kommer att l√§ggas till: {', '.join(missing_columns)}")
    
    # Bearbeta data f√∂r denna m√•nad, hoppa √∂ver sidor som redan finns om inte update_all=True
    all_data, ok, fail, skipped, updated = process_in_batches(
        page_list, cache, start_date, end_date, 
        existing_data=None if update_all else existing_data,
        missing_columns=None if update_all else missing_columns
    )
    
    # Spara resultaten
    if all_data:
        save_results(all_data, output_file)
        
        # Visa total r√§ckvidd och interaktioner f√∂r alla sidor med s√§ker summering
        try:
            # Anv√§nd safe_int_value f√∂r att f√∂rhindra typfel vid summering
            total_reach = sum(safe_int_value(item.get("Reach", 0)) for item in all_data)
            
            # Ber√§kna totaler f√∂r interaktioner om tillg√§ngligt
            has_engaged = any("Engaged Users" in item for item in all_data)
            has_engagements = any("Engagements" in item for item in all_data)
            has_reactions = any("Reactions" in item for item in all_data)
            has_clicks = any("Clicks" in item for item in all_data)
            has_publications = any("Publications" in item for item in all_data)  # Ny
            
            if has_engaged:
                total_engaged = sum(safe_int_value(item.get("Engaged Users", 0)) for item in all_data)
            else:
                total_engaged = 0
                
            if has_engagements:
                total_engagements = sum(safe_int_value(item.get("Engagements", 0)) for item in all_data)
            else:
                total_engagements = 0
                
            if has_reactions:
                total_reactions = sum(safe_int_value(item.get("Reactions", 0)) for item in all_data)
            else:
                total_reactions = 0
                
            if has_clicks:
                total_clicks = sum(safe_int_value(item.get("Clicks", 0)) for item in all_data)
            else:
                total_clicks = 0
                
            if has_publications:
                total_publications = sum(safe_int_value(item.get("Publications", 0)) for item in all_data)
            else:
                total_publications = 0
            
            logger.info(f"üìà Summering f√∂r {year}-{month:02d}:")
            logger.info(f"  - Total r√§ckvidd: {total_reach:,}")
            
            if has_engaged:
                logger.info(f"  - Engagerade anv√§ndare: {total_engaged:,}")
            if has_engagements:
                logger.info(f"  - Totala interaktioner: {total_engagements:,}")
            if has_reactions:
                logger.info(f"  - Reaktioner: {total_reactions:,}")
            if has_clicks:
                logger.info(f"  - Klick: {total_clicks:,}")
            if has_publications:
                logger.info(f"  - Publiceringar: {total_publications:,}")  # Ny
            
            if skipped > 0:
                logger.info(f"üìà {skipped} sidor hoppades √∂ver")
            if updated > 0:
                logger.info(f"üîÑ {updated} sidor uppdaterades med saknade kolumner")
                
            # Statusrapport om statuskolumn finns
            status_counts = {}
            for item in all_data:
                if "Status" in item:
                    status = item["Status"]
                    status_counts[status] = status_counts.get(status, 0) + 1
            
            if status_counts:
                logger.info(f"üìã Status√∂versikt:")
                for status, count in status_counts.items():
                    logger.info(f"  - {status}: {count} sidor")
            
            # Generera statusrapport om f√∂reg√•ende m√•nad finns
            if generate_status:
                previous_month = f"{year}-{month-1:02d}" if month > 1 else f"{year-1}-12"
                previous_file = f"FB_{previous_month.split('-')[0]}_{previous_month.split('-')[1]}.csv"
                
                if os.path.exists(previous_file):
                    logger.info(f"Genererar statusrapport genom att j√§mf√∂ra med {previous_file}")
                    try:
                        status_df = analyze_page_presence(previous_file, output_file)
                        save_status_report(status_df, year, month)
                    except Exception as e:
                        logger.error(f"Kunde inte generera statusrapport: {e}")
        
        except Exception as e:
            logger.error(f"Fel vid ber√§kning av summor: {e}")
        
        return True
    else:
        logger.warning(f"‚ö†Ô∏è Inga data att spara f√∂r {year}-{month:02d}")
        return False

def main():
    """Huvudfunktion f√∂r att k√∂ra hela processen"""
    # Parsa kommandoradsargument
    parser = argparse.ArgumentParser(description="Generera Facebook-r√§ckviddsrapport f√∂r alla sidor och m√•nader")
    
    # Datum-grupp f√∂r m√•nader
    date_group = parser.add_argument_group("Datumargument f√∂r m√•nader")
    date_group.add_argument("--start", help="Start√•r-m√•nad (YYYY-MM)")
    date_group.add_argument("--month", help="K√∂r endast f√∂r angiven m√•nad (YYYY-MM)")
    
    # Custom datumintervall
    custom_group = parser.add_argument_group("Custom datumintervall")
    custom_group.add_argument("--from", dest="from_date", help="Custom startdatum (YYYY-MM-DD)")
    custom_group.add_argument("--to", dest="to_date", help="Custom slutdatum (YYYY-MM-DD)")
    custom_group.add_argument("--current-month-so-far", action="store_true", 
                            help="H√§mta data fr√•n 1:a i m√•naden till idag")
    custom_group.add_argument("--last-n-days", type=int, metavar="N",
                            help="H√§mta data f√∂r senaste N dagar (inklusive idag)")
    custom_group.add_argument("--last-week", action="store_true", 
                            help="H√§mta data f√∂r senaste 7 dagar (inklusive idag)")
    custom_group.add_argument("--last-month", action="store_true", 
                            help="H√§mta data f√∂r senaste 30 dagar (inklusive idag)")
    
    # Operationsmodifikatorer
    ops_group = parser.add_argument_group("Operationsmodifikatorer")
    ops_group.add_argument("--update-all", action="store_true", 
                          help="Uppdatera alla sidor √§ven om de redan finns i CSV-filen")
    ops_group.add_argument("--check-new", action="store_true", 
                          help="Kontrollera efter nya sidor i alla befintliga m√•nader")
    ops_group.add_argument("--status", 
                          help="Generera endast statusrapport f√∂r angiven m√•nad (YYYY-MM)")
    ops_group.add_argument("--debug", action="store_true", 
                          help="Aktivera debug-loggning")
    
    args = parser.parse_args()
    
    # S√§tt debug-l√§ge om beg√§rt
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.debug("Debug-l√§ge aktiverat")
    
    # Kontrollera f√∂r inkompatibla argumentkombinationer
    date_args_count = sum([
        bool(args.start), bool(args.month), bool(args.from_date and args.to_date),
        args.current_month_so_far, bool(args.last_n_days), args.last_week, args.last_month,
        args.check_new, bool(args.status)
    ])
    
    if date_args_count > 1:
        logger.error("‚ùå Endast ett datumargument kan anv√§ndas √•t g√•ngen")
        parser.print_help()
        sys.exit(1)
    
    # Anv√§nd argument om de finns
    start_year_month = args.start or INITIAL_START_YEAR_MONTH
    
    logger.info(f"üìä Facebook Reach & Interactions Report Generator ‚Äì v2.4")
    logger.info(f"Startdatum: {start_year_month}")
    logger.info("-------------------------------------------------------------------")
    
    # Kontrollera token och varna om den snart g√•r ut
    check_token_expiry()
    
    # Validera token
    if not validate_token(ACCESS_TOKEN):
        logger.error("‚ùå Token kunde inte valideras. Avbryter.")
        return
    
    # Ladda cache f√∂r sidnamn
    cache = load_page_cache()
    
    # Om --status anv√§nds, generera endast statusrapport
    if args.status:
        try:
            year, month = map(int, args.status.split("-"))
            current_file = f"FB_{year}_{month:02d}.csv"
            
            if not os.path.exists(current_file):
                logger.error(f"‚ùå Fil {current_file} hittades inte. Kan inte generera statusrapport.")
                return
                
            # Best√§m f√∂reg√•ende m√•nad
            if month > 1:
                prev_month = month - 1
                prev_year = year
            else:
                prev_month = 12
                prev_year = year - 1
                
            prev_file = f"FB_{prev_year}_{prev_month:02d}.csv"
            
            if not os.path.exists(prev_file):
                logger.error(f"‚ùå Fil {prev_file} hittades inte. Kan inte j√§mf√∂ra med f√∂reg√•ende m√•nad.")
                return
                
            logger.info(f"Genererar statusrapport f√∂r {year}-{month:02d}")
            status_df = analyze_page_presence(prev_file, current_file)
            save_status_report(status_df, year, month)
            return
        except Exception as e:
            logger.error(f"‚ùå Fel vid generering av statusrapport: {e}")
            return
    
    # H√§mta alla tillg√§ngliga sidor (en g√•ng f√∂r alla k√∂rningar)
    page_list = get_page_ids_with_access(ACCESS_TOKEN)
    
    if not page_list:
        logger.error("‚ùå Inga sidor hittades. Avbryter.")
        return
    
    # Filtrera bort placeholder-sidor
    page_list = filter_placeholder_pages(page_list)
    
    if not page_list:
        logger.error("‚ùå Inga sidor kvar efter filtrering. Avbryter.")
        return
    
    # Hantera custom datumintervall
    start_date, end_date = parse_date_args(args)
    if start_date and end_date:
        logger.info(f"üóìÔ∏è K√∂r f√∂r custom datumintervall: {start_date} till {end_date}")
        process_custom_period(start_date, end_date, cache, page_list, update_all=args.update_all)
        save_page_cache(cache)
        return
    
    # Om check-new-argument, kontrollera alla befintliga m√•nader efter nya sidor
    if args.check_new:
        logger.info("Kontrollerar efter nya sidor i alla befintliga m√•nader...")
        existing_reports = get_existing_reports()
        
        for report in sorted(existing_reports):
            year, month = map(int, report.split("-"))
            logger.info(f"Kontrollerar {year}-{month:02d} efter nya sidor...")
            process_month(year, month, cache, page_list, update_all=args.update_all, generate_status=True)
            
        logger.info("‚úÖ Kontroll efter nya sidor slutf√∂rd")
        save_page_cache(cache)
        return
    
    # Om specifik m√•nad angivits, k√∂r endast den
    if args.month:
        try:
            year, month = map(int, args.month.split("-"))
            logger.info(f"K√∂r endast f√∂r specifik m√•nad: {year}-{month:02d}")
            process_month(year, month, cache, page_list, update_all=args.update_all, generate_status=True)
            save_page_cache(cache)
            return
        except ValueError:
            logger.error(f"Ogiltigt m√•nadsformat: {args.month}. Anv√§nd YYYY-MM.")
            return
    
    # H√§mta befintliga rapporter
    existing_reports = get_existing_reports()
    logger.info(f"Hittade {len(existing_reports)} befintliga rapporter: {', '.join(sorted(existing_reports)) if existing_reports else 'Inga'}")
    
    # F√• saknade m√•nader
    missing_months = get_missing_months(existing_reports, start_year_month)
    
    if not missing_months:
        logger.info("‚úÖ Alla m√•nader √§r redan bearbetade. Inget att g√∂ra.")
        logger.info("Om du vill kontrollera efter nya sidor i befintliga rapporter, anv√§nd --check-new")
        return
    
    logger.info(f"Beh√∂ver bearbeta {len(missing_months)} saknade m√•nader: {', '.join([f'{y}-{m:02d}' for y, m in missing_months])}")
    
    # Bearbeta varje saknad m√•nad
    for year, month in missing_months:
        logger.info(f"‚è≥ Bearbetar data f√∂r {year}-{month:02d}...")
        
        # Bearbeta denna m√•nad
        success = process_month(year, month, cache, page_list, update_all=args.update_all, generate_status=True)
        
        # Spara cache efter varje m√•nad
        save_page_cache(cache)
        
        if not success:
            logger.warning(f"‚ö†Ô∏è Kunde inte slutf√∂ra bearbetningen f√∂r {year}-{month:02d}")
        else:
            logger.info(f"‚úÖ Slutf√∂rde bearbetningen f√∂r {year}-{month:02d}")
        
        # Pausa f√∂r att respektera API-begr√§nsningar om det finns fler m√•nader att bearbeta
        if missing_months.index((year, month)) < len(missing_months) - 1:
            logger.info(f"Pausar i {MONTH_PAUSE_SECONDS} sekunder f√∂r att respektera API-begr√§nsningar...")
            time.sleep(MONTH_PAUSE_SECONDS)
    
    # Visa statistik om API-anv√§ndning
    elapsed_time = time.time() - start_time
    logger.info(f"‚è±Ô∏è Total k√∂rtid: {elapsed_time:.1f} sekunder")
    logger.info(f"üåê API-anrop: {api_call_count} ({api_call_count/elapsed_time*3600:.1f}/timme)")
    logger.info(f"‚úÖ Klar! Bearbetade {len(missing_months)} m√•nader")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("Avbruten av anv√§ndare. Sparar mellanlagrat arbete...")
        # H√§r kunde vi implementera att spara framsteg
        sys.exit(1)
    except Exception as e:
        logger.critical(f"Ov√§ntat fel: {e}")
        import traceback
        logger.critical(traceback.format_exc())
        sys.exit(1)