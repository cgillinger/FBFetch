# diagnostics.py
# Komplett diagnostikskript f√∂r Facebook r√§ckviddsm√§tningar

import csv
import json
import os
import time
import requests
import logging
import argparse
import sys
import glob
from datetime import datetime, timedelta
from calendar import monthrange
from config import (
    ACCESS_TOKEN, TOKEN_LAST_UPDATED, INITIAL_START_YEAR_MONTH,
    API_VERSION, CACHE_FILE, 
    BATCH_SIZE, MAX_RETRIES, RETRY_DELAY, 
    TOKEN_VALID_DAYS, MAX_REQUESTS_PER_HOUR,
    MONTH_PAUSE_SECONDS
)

# Konfigurera loggning
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("facebook_reach_diagnostic.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# R√§knare f√∂r API-anrop
api_call_count = 0
start_time = time.time()

def check_token_expiry():
    """Kontrollera om token snart g√•r ut och varna anv√§ndaren"""
    try:
        last_updated = datetime.strptime(TOKEN_LAST_UPDATED, "%Y-%m-%d")
        days_since = (datetime.now() - last_updated).days
        days_left = TOKEN_VALID_DAYS - days_since
        
        logger.info(f"üîë Token skapades f√∂r {days_since} dagar sedan ({days_left} dagar kvar till utg√•ng).")
        
        if days_left <= 7:
            logger.warning(f"‚ö†Ô∏è VARNING: Din token g√•r ut inom {days_left} dagar! Skapa en ny token snart.")
        elif days_left <= 0:
            logger.error(f"‚ùå KRITISKT: Din token har g√•tt ut! Skapa en ny token omedelbart.")
            sys.exit(1)
    except Exception as e:
        logger.error(f"‚ö†Ô∏è Kunde inte tolka TOKEN_LAST_UPDATED: {e}")

def load_page_cache():
    """Ladda cache med sidnamn f√∂r att minska API-anrop"""
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                logger.debug(f"Laddar sid-cache fr√•n {CACHE_FILE}")
                return json.load(f)
        except json.JSONDecodeError:
            logger.warning(f"Kunde inte ladda cache-fil, skapar ny cache")
    return {}

def save_page_cache(cache):
    """Spara cache med sidnamn f√∂r framtida k√∂rningar"""
    try:
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
            logger.debug(f"Sparade sid-cache till {CACHE_FILE}")
    except Exception as e:
        logger.error(f"Kunde inte spara cache: {e}")

def api_request(url, params, retries=MAX_RETRIES):
    """G√∂r API-f√∂rfr√•gan med √•terf√∂rs√∂k och rate limit-hantering"""
    global api_call_count
    
    # Kontrollera om vi n√§rmar oss rate limit
    current_time = time.time()
    elapsed_hours = (current_time - start_time) / 3600
    rate = api_call_count / elapsed_hours if elapsed_hours > 0 else 0
    
    if rate > MAX_REQUESTS_PER_HOUR * 0.9:  # Om vi anv√§nt 90% av rate limit
        wait_time = 3600 / MAX_REQUESTS_PER_HOUR  # V√§nta tillr√§ckligt f√∂r att h√•lla oss under gr√§nsen
        logger.warning(f"N√§rmar oss rate limit ({int(rate)}/h). V√§ntar {wait_time:.1f} sekunder...")
        time.sleep(wait_time)
    
    for attempt in range(retries):
        try:
            api_call_count += 1
            response = requests.get(url, params=params, timeout=30)
            
            # Hantera vanliga HTTP-fel
            if response.status_code == 429:  # Too Many Requests
                retry_after = int(response.headers.get('Retry-After', RETRY_DELAY))
                logger.warning(f"Rate limit n√•tt! V√§ntar {retry_after} sekunder... (f√∂rs√∂k {attempt+1}/{retries})")
                time.sleep(retry_after)
                continue
                
            elif response.status_code >= 500:  # Server error
                wait_time = RETRY_DELAY * (2 ** attempt)  # Exponentiell backoff
                logger.warning(f"Serverfel: {response.status_code}. V√§ntar {wait_time} sekunder... (f√∂rs√∂k {attempt+1}/{retries})")
                time.sleep(wait_time)
                continue
                
            elif response.status_code == 400:  # Bad Request
                data = response.json()
                if "error" in data:
                    error_code = data["error"].get("code")
                    error_msg = data["error"].get("message", "Ok√§nt fel")
                    
                    # Hantera specifika felkoder
                    if error_code == 4:  # App-specifikt rate limit
                        wait_time = 60 * (attempt + 1)  # V√§nta l√§ngre f√∂r varje f√∂rs√∂k
                        logger.warning(f"App rate limit: {error_msg}. V√§ntar {wait_time} sekunder...")
                        time.sleep(wait_time)
                        continue
                        
                    elif error_code == 190:  # Ogiltig token
                        logger.error(f"Access token ogiltig: {error_msg}")
                        return None
                        
            # Om allt ovan misslyckas och responskoden fortfarande √§r en felsignal
            if response.status_code != 200:
                logger.error(f"HTTP-fel {response.status_code}: {response.text}")
                if attempt < retries - 1:
                    wait_time = RETRY_DELAY * (2 ** attempt)
                    logger.info(f"V√§ntar {wait_time} sekunder innan nytt f√∂rs√∂k... (f√∂rs√∂k {attempt+1}/{retries})")
                    time.sleep(wait_time)
                    continue
                return None
                
            # Analysera JSON-svaret
            try:
                return response.json()
            except json.JSONDecodeError:
                logger.error(f"Kunde inte tolka JSON-svar: {response.text[:100]}")
                return None
                
        except requests.RequestException as e:
            logger.error(f"N√§tverksfel: {e}")
            if attempt < retries - 1:
                wait_time = RETRY_DELAY * (2 ** attempt)
                logger.info(f"V√§ntar {wait_time} sekunder innan nytt f√∂rs√∂k... (f√∂rs√∂k {attempt+1}/{retries})")
                time.sleep(wait_time)
            else:
                return None
    
    return None

def validate_token(token):
    """Validera att token √§r giltig och h√§mta anv√§ndarbeh√∂righeter"""
    logger.info("Validerar token...")
    url = f"https://graph.facebook.com/{API_VERSION}/debug_token"
    params = {"input_token": token, "access_token": token}
    
    data = api_request(url, params)
    
    if not data or "data" not in data:
        logger.error("‚ùå Kunde inte validera token")
        return False
        
    if not data["data"].get("is_valid"):
        logger.error(f"‚ùå Token √§r ogiltig: {data['data'].get('error', {}).get('message', 'Ok√§nd anledning')}")
        return False
        
    logger.info(f"‚úÖ Token validerad. App ID: {data['data'].get('app_id')}")
    return True

def get_page_ids_with_access(token):
    """H√§mta alla sidor som token har √•tkomst till"""
    logger.info("H√§mtar tillg√§ngliga sidor...")
    url = f"https://graph.facebook.com/{API_VERSION}/me/accounts"
    params = {"access_token": token, "limit": 100, "fields": "id,name"}
    
    pages = []
    next_url = url
    
    while next_url:
        data = api_request(url if next_url == url else next_url, {} if next_url != url else params)
        
        if not data or "data" not in data:
            break
            
        pages.extend(data["data"])
        logger.debug(f"Hittade {len(data['data'])} sidor i denna batch")
        
        # Hantera paginering
        next_url = data.get("paging", {}).get("next")
        if next_url and next_url != url:
            logger.debug(f"H√§mtar n√§sta sida fr√•n: {next_url}")
        else:
            break
    
    if not pages:
        logger.warning("Inga sidor hittades. Token kanske saknar 'pages_show_list'-beh√∂righet.")
    
    page_ids = [(page["id"], page["name"]) for page in pages]
    logger.info(f"‚úÖ Hittade {len(page_ids)} sidor att analysera")
    return page_ids

def get_page_name(page_id, cache):
    """H√§mta sidans namn fr√•n cache eller API"""
    if page_id in cache:
        return cache[page_id]
    
    logger.debug(f"H√§mtar namn f√∂r sida {page_id}...")
    url = f"https://graph.facebook.com/{API_VERSION}/{page_id}"
    params = {"fields": "name", "access_token": ACCESS_TOKEN}
    
    data = api_request(url, params)
    
    if not data or "error" in data:
        error_msg = data.get("error", {}).get("message", "Ok√§nt fel") if data else "Fel vid API-anrop"
        logger.warning(f"‚ö†Ô∏è Kunde inte h√§mta namn f√∂r sida {page_id}: {error_msg}")
        return None
    
    name = data.get("name", f"Page {page_id}")
    cache[page_id] = name
    return name

def get_page_access_token(page_id, system_token):
    """Konvertera systemanv√§ndartoken till en Page Access Token f√∂r en specifik sida"""
    logger.debug(f"H√§mtar Page Access Token f√∂r sida {page_id}...")
    url = f"https://graph.facebook.com/{API_VERSION}/{page_id}"
    params = {
        "fields": "access_token",
        "access_token": system_token
    }
    
    data = api_request(url, params)
    
    if not data or "error" in data or "access_token" not in data:
        error_msg = data.get("error", {}).get("message", "Ok√§nt fel") if data and "error" in data else "Kunde inte h√§mta token"
        logger.warning(f"‚ö†Ô∏è Kunde inte h√§mta Page Access Token f√∂r sida {page_id}: {error_msg}")
        return None
    
    return data["access_token"]

def get_single_metric(page_id, page_token, since, until, metric_name, period="total_over_range"):
    """H√§mta ett enskilt m√§tv√§rde fr√•n Facebook API"""
    url = f"https://graph.facebook.com/{API_VERSION}/{page_id}/insights"
    params = {
        "access_token": page_token,
        "since": since,
        "until": until,
        "period": period,
        "metric": metric_name
    }
    
    data = api_request(url, params)
    
    if not data or "error" in data:
        if data and "error" in data:
            error_msg = data["error"].get("message", "Ok√§nt fel")
            if "must be a valid insights metric" in error_msg:
                logger.debug(f"Metriken '{metric_name}' finns inte f√∂r sida {page_id} (vanligt f√∂r mindre sidor)")
            else:
                logger.debug(f"Kunde inte h√§mta {metric_name} f√∂r sida {page_id}: {error_msg}")
        return 0
    
    if "data" not in data or not data["data"]:
        return 0
    
    # Extrahera v√§rdet fr√•n svaret
    for item in data["data"]:
        if item.get("name") == metric_name and item.get("values") and len(item["values"]) > 0:
            return item["values"][0].get("value", 0)
    
    return 0

def process_month_diagnostic(year, month, test_metrics):
    """K√∂r diagnostik f√∂r en m√•nad med olika m√§tv√§rden"""
    # S√§tt datumintervall f√∂r m√•naden
    start_date = f"{year}-{month:02d}-01"
    last_day = monthrange(year, month)[1]
    end_date = f"{year}-{month:02d}-{last_day}"
    
    logger.info(f"Diagnostisk k√∂rning f√∂r {year}-{month:02d} fr√•n {start_date} till {end_date}")
    
    # H√§mta sidlista
    page_list = get_page_ids_with_access(ACCESS_TOKEN)
    if not page_list:
        logger.error("‚ùå Inga sidor hittades. Avbryter.")
        return False
    
    # Skapa en cache f√∂r sidnamn
    cache = load_page_cache()
    
    # F√∂rbered resultatlista f√∂r varje m√§tv√§rde
    results_by_metric = {metric: [] for metric in test_metrics.keys()}
    
    # Bearbeta alla sidor
    total_pages = len(page_list)
    success = 0
    failed = 0
    
    for i, (page_id, page_name) in enumerate(page_list):
        try:
            name = page_name or get_page_name(page_id, cache)
            if not name:
                logger.warning(f"‚ö†Ô∏è Kunde inte hitta namn f√∂r sida {page_id}, hoppar √∂ver")
                failed += 1
                continue
            
            logger.info(f"üìä H√§mtar diagnostikdata f√∂r: {name} (ID: {page_id}) [{i+1}/{total_pages}]")
            
            # H√§mta page token
            page_token = get_page_access_token(page_id, ACCESS_TOKEN)
            if not page_token:
                logger.warning(f"‚ö†Ô∏è Kunde inte h√§mta token f√∂r sida {page_id}, hoppar √∂ver")
                failed += 1
                continue
            
            # Testa varje m√§tv√§rde separat
            page_results = {"Page": name, "Page ID": page_id}
            
            for metric_key, metric_details in test_metrics.items():
                metric_name = metric_details["api_name"]
                try:
                    value = get_single_metric(page_id, page_token, start_date, end_date, metric_name)
                    page_results[metric_key] = value
                    logger.debug(f"  - {metric_key}: {value}")
                except Exception as e:
                    logger.warning(f"Kunde inte h√§mta {metric_key} f√∂r {name}: {e}")
                    page_results[metric_key] = 0
            
            # L√§gg till resultaten i respektive lista
            for metric_key in test_metrics.keys():
                results_by_metric[metric_key].append({
                    "Page": name,
                    "Page ID": page_id,
                    "Reach": page_results[metric_key]  # Anv√§nd detta m√§tv√§rde som "Reach"
                })
            
            success += 1
            
            # Visa framsteg
            progress = (i + 1) / total_pages * 100
            logger.info(f"Framsteg: {progress:.1f}% klar ({success} lyckade, {failed} misslyckade)")
            
        except Exception as e:
            logger.error(f"Fel vid bearbetning av sida {page_id}: {e}")
            failed += 1
    
    # Spara resultat f√∂r varje m√§tv√§rde till separata filer
    for metric_key, results in results_by_metric.items():
        if results:
            output_file = f"FB_{year}_{month:02d}_{metric_key}.csv"
            try:
                # Sortera resultaten efter r√§ckvidd (h√∂gst f√∂rst)
                sorted_data = sorted(results, key=lambda x: x.get("Reach", 0), reverse=True)
                
                with open(output_file, mode="w", newline="", encoding="utf-8") as f:
                    writer = csv.DictWriter(f, fieldnames=["Page", "Page ID", "Reach"])
                    writer.writeheader()
                    writer.writerows(sorted_data)
                    
                total_reach = sum(item.get("Reach", 0) for item in results)
                logger.info(f"‚úÖ Sparade data f√∂r {metric_key} till {output_file} (Total: {total_reach:,})")
            except Exception as e:
                logger.error(f"‚ùå Kunde inte spara data f√∂r {metric_key}: {e}")
    
    # Spara cache
    save_page_cache(cache)
    
    # Skapa en j√§mf√∂relsefil
    create_comparison_report(year, month, test_metrics, results_by_metric)
    
    return True

def create_comparison_report(year, month, test_metrics, results_by_metric):
    """Skapa en sammanfattande j√§mf√∂relserapport"""
    output_file = f"FB_{year}_{month:02d}_comparison.csv"
    
    try:
        # F√∂rbereda j√§mf√∂relsedata
        comparison_data = []
        
        # Samla alla page_ids fr√•n alla resultat
        all_page_ids = set()
        for results in results_by_metric.values():
            all_page_ids.update(item["Page ID"] for item in results)
        
        # Skapa en mapp fr√•n page_id till sidnamn
        name_map = {}
        for results in results_by_metric.values():
            for item in results:
                name_map[item["Page ID"]] = item["Page"]
        
        # Skapa j√§mf√∂relsedata f√∂r varje sida
        for page_id in all_page_ids:
            page_data = {"Page": name_map.get(page_id, f"Page {page_id}"), "Page ID": page_id}
            
            for metric_key in test_metrics.keys():
                # Hitta r√§ckvidd f√∂r denna sida i detta m√§tv√§rdes resultat
                metric_results = results_by_metric[metric_key]
                matching_results = [item for item in metric_results if item["Page ID"] == page_id]
                
                if matching_results:
                    page_data[metric_key] = matching_results[0]["Reach"]
                else:
                    page_data[metric_key] = 0
            
            comparison_data.append(page_data)
        
        # Sortera efter det f√∂rsta m√§tv√§rdet (h√∂gst f√∂rst)
        first_metric = next(iter(test_metrics.keys()))
        sorted_data = sorted(comparison_data, key=lambda x: x.get(first_metric, 0), reverse=True)
        
        # Spara j√§mf√∂relsefil
        fieldnames = ["Page", "Page ID"] + list(test_metrics.keys())
        with open(output_file, mode="w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(sorted_data)
        
        logger.info(f"‚úÖ Sparade j√§mf√∂relserapport till {output_file}")
        
        # Ber√§kna totaler f√∂r varje m√§tv√§rde
        totals = {}
        for metric_key in test_metrics.keys():
            totals[metric_key] = sum(item.get(metric_key, 0) for item in comparison_data)
        
        logger.info("J√§mf√∂relse av totala r√§ckvidder:")
        for metric_key, total in totals.items():
            description = test_metrics[metric_key]["description"]
            logger.info(f"  - {description}: {total:,}")
        
    except Exception as e:
        logger.error(f"‚ùå Kunde inte skapa j√§mf√∂relserapport: {e}")

def main():
    """Huvudfunktion f√∂r att k√∂ra diagnostik"""
    # Parsa kommandoradsargument
    parser = argparse.ArgumentParser(description="Diagnostisk k√∂rning av Facebook-r√§ckviddsm√•tt")
    parser.add_argument("--start", help="Start√•r-m√•nad (YYYY-MM)")
    parser.add_argument("--month", help="Specifik m√•nad att testa (YYYY-MM)")
    parser.add_argument("--debug", action="store_true", help="Aktivera debug-loggning")
    args = parser.parse_args()
    
    # S√§tt debug-l√§ge om beg√§rt
    if args.debug:
        logger.setLevel(logging.DEBUG)
        logger.debug("Debug-l√§ge aktiverat")
    
    logger.info(f"üìä Facebook Reach Diagnostic Tool")
    logger.info("-------------------------------------------------------------------")
    
    # Kontrollera token och varna om den snart g√•r ut
    check_token_expiry()
    
    # Validera token
    if not validate_token(ACCESS_TOKEN):
        logger.error("‚ùå Token kunde inte valideras. Avbryter.")
        return
    
    # Definiera m√§tv√§rden att testa
    test_metrics = {
        "total_unique": {
            "api_name": "page_impressions_unique",
            "description": "Unika visningar (total r√§ckvidd)"
        },
        "organic_unique": {
            "api_name": "page_impressions_unique_organic",
            "description": "Organisk r√§ckvidd (unika anv√§ndare)"
        },
        "total_impressions": {
            "api_name": "page_impressions",
            "description": "Totala visningar (inklusive upprepade)"
        },
        "page_engaged_users": {
            "api_name": "page_engaged_users",
            "description": "Engagerade anv√§ndare"
        }
    }
    
    # Best√§m vilken m√•nad att diagnostisera
    if args.month:
        try:
            year, month = map(int, args.month.split("-"))
            process_month_diagnostic(year, month, test_metrics)
        except ValueError:
            logger.error(f"‚ùå Ogiltigt m√•nadsformat: {args.month}. Anv√§nd YYYY-MM (t.ex. 2025-01)")
    else:
        # Anv√§nd senaste avslutade m√•naden
        now = datetime.now()
        if now.month == 1:
            year, month = now.year - 1, 12
        else:
            year, month = now.year, now.month - 1
        
        logger.info(f"Ingen m√•nad specificerad, anv√§nder senaste avslutade m√•naden: {year}-{month:02d}")
        process_month_diagnostic(year, month, test_metrics)
    
    logger.info("‚úÖ Diagnostik slutf√∂rd!")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("Avbruten av anv√§ndare.")
        sys.exit(1)
    except Exception as e:
        logger.critical(f"Ov√§ntat fel: {e}")
        import traceback
        logger.critical(traceback.format_exc())
        sys.exit(1)
