# demographics.py
#
# Ett verktyg f√∂r att h√§mta demografisk data fr√•n Facebook-sidor
# Skapar en Excel-fil d√§r varje sida har en egen flik med demografisk information
#
# Anv√§ndning:
#   python demographics.py [--output FILNAMN] [--pages SIDA1,SIDA2,...] [--detailed]
#

import os
import sys
import time
import json
import argparse
import logging
import requests
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
from openpyxl.utils import get_column_letter

# Importera konfigurationsvariabler fr√•n config.py
try:
    from config import (
        ACCESS_TOKEN, TOKEN_LAST_UPDATED, API_VERSION, 
        MAX_RETRIES, RETRY_DELAY
    )
except ImportError:
    print("Fel: Kan inte hitta config.py. Se till att filen finns i samma mapp.")
    sys.exit(1)

# Definition av demografiska metriker att h√§mta och deras perioder
# Viktigt: Olika metriker har olika giltiga perioder enligt Facebook API
DEMOGRAPHIC_METRICS = {
    # Fans-metriker (kr√§ver "lifetime" period)
    "page_fans_city": {
        "name": "Fans per stad",
        "periods": ["lifetime"],
        "description": "Aggregerad data om fans per stad"
    },
    "page_fans_country": {
        "name": "Fans per land",
        "periods": ["lifetime"],
        "description": "Aggregerad data om fans per land"
    },
    "page_fans_gender_age": {
        "name": "Fans per k√∂n och √•lder",
        "periods": ["lifetime"],
        "description": "Aggregerad data om fans per k√∂n och √•ldersgrupp"
    },
    "page_fans_locale": {
        "name": "Fans per spr√•k",
        "periods": ["lifetime"],
        "description": "Aggregerad data om fans per spr√•kinst√§llning"
    },
    "page_fans_online_per_day": {
        "name": "Fans online per dag",
        "periods": ["lifetime"],
        "description": "N√§r fans √§r online per veckodag"
    },
    "page_fans_online": {
        "name": "Fans online per timme",
        "periods": ["lifetime"],
        "description": "N√§r fans √§r online per timme p√• dygnet"
    },
    
    # R√§ckviddsmetriker (fungerar med dag, vecka, 28 dagar)
    "page_impressions_by_city_unique": {
        "name": "R√§ckvidd per stad",
        "periods": ["day", "week", "days_28"],
        "description": "Unika personer som n√•tts av inneh√•ll, per stad"
    },
    "page_impressions_by_country_unique": {
        "name": "R√§ckvidd per land",
        "periods": ["day", "week", "days_28"],
        "description": "Unika personer som n√•tts av inneh√•ll, per land"
    },
    "page_impressions_by_age_gender_unique": {
        "name": "R√§ckvidd per √•lder/k√∂n",
        "periods": ["day", "week", "days_28"],
        "description": "Unika personer som n√•tts av inneh√•ll, per √•lder och k√∂n"
    },
    
    # Interaktionsmetriker
    "page_engaged_users": {
        "name": "Engagerade anv√§ndare",
        "periods": ["day", "week", "days_28"],
        "description": "Unika anv√§ndare som interagerat med sidan"
    },
    
    # Placering/Enhetsmetriker
    "page_impressions_by_browser_unique": {
        "name": "R√§ckvidd per webbl√§sare",
        "periods": ["day", "week", "days_28"],
        "description": "Unika personer som n√•tts per webbl√§sare"
    },
    "page_impressions_by_device_type_unique": {
        "name": "R√§ckvidd per enhetstyp",
        "periods": ["day", "week", "days_28"],
        "description": "Unika personer som n√•tts per enhetstyp (desktop, mobil, etc.)"
    },
    
    # Tillg√§nglig endast f√∂r st√∂rre sidor med minst 100 personer i kategorin
    "page_content_activity_by_age_gender_unique": {
        "name": "Aktivitet per √•lder/k√∂n",
        "periods": ["day", "week", "days_28"],
        "description": "Unika personer som interagerat med inneh√•ll per √•lder och k√∂n"
    }
}

# Konfigurera loggning
def setup_logging():
    """Konfigurera loggning med datumst√§mplad loggfil"""
    now = datetime.now()
    log_dir = "logs"
    
    # Skapa loggdirektory om den inte finns
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    # Skapa datumst√§mplad loggfilnamn
    log_filename = os.path.join(log_dir, f"demographics_{now.strftime('%Y-%m-%d_%H-%M-%S')}.log")
    
    # Konfigurera loggning
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename),
            logging.FileHandler("demographics.log", mode="w"),  # √ñverskriver tidigare loggfil
            logging.StreamHandler()  # Terminal-utskrift
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"Startar loggning till fil: {log_filename}")
    
    return logger

# Konfigurera loggning
logger = setup_logging()

# R√§knare f√∂r API-anrop
api_call_count = 0
start_time = time.time()

def check_token_expiry():
    """Kontrollera om token snart g√•r ut och varna anv√§ndaren"""
    try:
        last_updated = datetime.strptime(TOKEN_LAST_UPDATED, "%Y-%m-%d")
        TOKEN_VALID_DAYS = 60  # Meta tokens √§r vanligtvis giltiga i 60 dagar
        days_since = (datetime.now() - last_updated).days
        days_left = TOKEN_VALID_DAYS - days_since
        
        logger.info(f"üîë Token skapades f√∂r {days_since} dagar sedan ({days_left} dagar kvar till utg√•ng).")
        
        if days_left <= 7:
            logger.warning(f"‚ö†Ô∏è VARNING: Din token g√•r ut inom {days_left} dagar! Skapa en ny token snart.")
        elif days_left <= 0:
            logger.error(f"‚ùå KRITISKT: Din token har g√•tt ut! Skapa en ny token omedelbart.")
            sys.exit(1)
    except Exception as e:
        logger.error(f"‚ö†Ô∏è Kunde inte tolka TOKEN_LAST_UPDATED: {e}")

def api_request(url, params, retries=MAX_RETRIES):
    """G√∂r API-f√∂rfr√•gan med √•terf√∂rs√∂k och rate limit-hantering"""
    global api_call_count
    
    for attempt in range(retries):
        try:
            api_call_count += 1
            logger.debug(f"API-anrop {api_call_count}: {url} med parametrar {params}")
            response = requests.get(url, params=params, timeout=30)
            
            # Kontrollera X-App-Usage och X-Ad-Account-Usage headers f√∂r b√§ttre rate limiting
            app_usage = response.headers.get('X-App-Usage')
            if app_usage:
                try:
                    usage_data = json.loads(app_usage)
                    call_count = usage_data.get('call_count', 0)
                    total_time = usage_data.get('total_time', 0)
                    total_cputime = usage_data.get('total_cputime', 0)
                    
                    # Om vi n√§rmar oss gr√§nser, v√§nta
                    if call_count > 80 or total_time > 80 or total_cputime > 80:  # 80% av gr√§nsen
                        wait_time = 60  # V√§nta 1 minut
                        logger.warning(f"API-anv√§ndning h√∂g: {app_usage}. V√§ntar {wait_time} sekunder...")
                        time.sleep(wait_time)
                except (json.JSONDecodeError, KeyError):
                    logger.debug(f"Kunde inte tolka X-App-Usage: {app_usage}")
            
            # Hantera vanliga HTTP-fel
            if response.status_code == 429:  # Too Many Requests
                retry_after = int(response.headers.get('Retry-After', RETRY_DELAY))
                logger.warning(f"Rate limit n√•tt! V√§ntar {retry_after} sekunder... (f√∂rs√∂k {attempt+1}/{retries})")
                time.sleep(retry_after)
                continue
                
            elif response.status_code >= 500:  # Server error
                wait_time = RETRY_DELAY * (2 ** attempt)  # Exponentiell backoff
                logger.warning(f"Serverfel: {response.status_code}. V√§ntar {wait_time} sekunder... (f√∂rs√∂k {attempt+1}/{retries})")
                time.sleep(wait_time)
                continue
            
            # F√∂r alla HTTP-svarkoder, f√∂rs√∂k tolka JSON-inneh√•llet
            try:
                json_data = response.json()
                
                # S√§rskild hantering f√∂r 400-fel (Bad Request)
                if response.status_code == 400 and "error" in json_data:
                    error_code = json_data["error"].get("code")
                    error_msg = json_data["error"].get("message", "Ok√§nt fel")
                    
                    # Hantera specifika felkoder
                    if error_code == 4:  # App-specifikt rate limit
                        wait_time = 60 * (attempt + 1)  # V√§nta l√§ngre f√∂r varje f√∂rs√∂k
                        logger.warning(f"App rate limit: {error_msg}. V√§ntar {wait_time} sekunder...")
                        time.sleep(wait_time)
                        continue
                        
                    elif error_code == 100 and "valid insights metric" in error_msg:
                        # Detta √§r ett f√∂rv√§ntat fel om metriken inte st√∂ds, returnera felmeddelandet
                        logger.debug(f"Metrik inte tillg√§nglig: {error_msg}")
                        return json_data
                        
                    elif error_code == 190:  # Ogiltig token
                        logger.error(f"Access token ogiltig: {error_msg}")
                        return None
                
                # Om vi kommer hit och har en icke-200 status, logga felet men returnera √§nd√• JSON-data
                if response.status_code != 200:
                    logger.debug(f"HTTP-fel {response.status_code}: {response.text}")
                    
                    if attempt < retries - 1 and error_code != 100:  # F√∂rs√∂k inte igen f√∂r metriska fel
                        wait_time = RETRY_DELAY * (2 ** attempt)
                        logger.info(f"V√§ntar {wait_time} sekunder innan nytt f√∂rs√∂k... (f√∂rs√∂k {attempt+1}/{retries})")
                        time.sleep(wait_time)
                        continue
                    
                    # Returnera √§nd√• JSON-data s√• att anropande funktion kan hantera felet
                    return json_data
                
                # Allt gick bra, returnera data
                return json_data
                
            except json.JSONDecodeError:
                logger.error(f"Kunde inte tolka JSON-svar: {response.text[:100]}")
                if attempt < retries - 1:
                    wait_time = RETRY_DELAY * (2 ** attempt)
                    logger.info(f"V√§ntar {wait_time} sekunder innan nytt f√∂rs√∂k... (f√∂rs√∂k {attempt+1}/{retries})")
                    time.sleep(wait_time)
                    continue
                return None
                
        except requests.RequestException as e:
            logger.error(f"N√§tverksfel: {e}")
            if attempt < retries - 1:
                wait_time = RETRY_DELAY * (2 ** attempt)
                logger.info(f"V√§ntar {wait_time} sekunder innan nytt f√∂rs√∂k... (f√∂rs√∂k {attempt+1}/{retries})")
                time.sleep(wait_time)
            else:
                return None
    
    return None

def validate_token(token):
    """Validera att token √§r giltig och h√§mta anv√§ndarbeh√∂righeter"""
    logger.info("Validerar token...")
    url = f"https://graph.facebook.com/{API_VERSION}/debug_token"
    params = {"input_token": token, "access_token": token}
    
    data = api_request(url, params)
    
    if not data or "data" not in data:
        logger.error("‚ùå Kunde inte validera token")
        return False
        
    if not data["data"].get("is_valid"):
        logger.error(f"‚ùå Token √§r ogiltig: {data['data'].get('error', {}).get('message', 'Ok√§nd anledning')}")
        return False
        
    # Kontrollera beh√∂righeter
    permissions = data["data"].get("scopes", [])
    logger.info(f"‚úÖ Token validerad. App ID: {data['data'].get('app_id')}")
    
    # Kolla kritiska beh√∂righeter 
    required_permissions = {"read_insights", "pages_read_engagement"}
    missing_permissions = required_permissions - set(permissions)
    if missing_permissions:
        logger.warning(f"‚ö†Ô∏è Token saknar f√∂ljande beh√∂righeter: {', '.join(missing_permissions)}")
        logger.warning("Detta kan begr√§nsa vilken data som kan h√§mtas.")
    else:
        logger.info("‚úÖ Token har alla n√∂dv√§ndiga beh√∂righeter")
    
    return True

def get_page_ids_with_access(token):
    """H√§mta alla sidor som token har √•tkomst till"""
    logger.info("H√§mtar tillg√§ngliga sidor...")
    url = f"https://graph.facebook.com/{API_VERSION}/me/accounts"
    params = {"access_token": token, "limit": 100, "fields": "id,name,category,fan_count"}
    
    pages = []
    next_url = url
    
    while next_url:
        data = api_request(url if next_url == url else next_url, {} if next_url != url else params)
        
        if not data or "data" not in data:
            break
            
        pages.extend(data["data"])
        logger.debug(f"Hittade {len(data['data'])} sidor i denna batch")
        
        # Hantera paginering
        next_url = data.get("paging", {}).get("next")
        if next_url and next_url != url:
            logger.debug(f"H√§mtar n√§sta sida fr√•n: {next_url}")
        else:
            break
    
    if not pages:
        logger.warning("Inga sidor hittades. Token kanske saknar 'pages_show_list'-beh√∂righet.")
        return []
    
    # Sortera sidor efter antal fans (h√∂gst f√∂rst)
    pages.sort(key=lambda p: p.get("fan_count", 0), reverse=True)
    
    # Skapa en lista av tupler med (id, namn, kategori, antal fans)
    page_info = [(page["id"], 
                 page["name"], 
                 page.get("category", "Ok√§nd kategori"), 
                 page.get("fan_count", 0)) for page in pages]
    
    logger.info(f"‚úÖ Hittade {len(page_info)} sidor att analysera")
    
    # Visa de 5 st√∂rsta sidorna
    if len(page_info) > 0:
        logger.info("St√∂rsta sidor:")
        for i, (page_id, name, category, fan_count) in enumerate(page_info[:5], 1):
            logger.info(f"  {i}. {name} ({category}) - {fan_count:,} fans")
    
    return page_info

def get_page_access_token(page_id, system_token):
    """Konvertera systemanv√§ndartoken till en Page Access Token f√∂r en specifik sida"""
    logger.debug(f"H√§mtar Page Access Token f√∂r sida {page_id}...")
    url = f"https://graph.facebook.com/{API_VERSION}/{page_id}"
    params = {
        "fields": "access_token",
        "access_token": system_token
    }
    
    data = api_request(url, params)
    
    if not data or "error" in data or "access_token" not in data:
        error_msg = data.get("error", {}).get("message", "Ok√§nt fel") if data and "error" in data else "Kunde inte h√§mta token"
        logger.warning(f"‚ö†Ô∏è Kunde inte h√§mta Page Access Token f√∂r sida {page_id}: {error_msg}")
        return None
    
    return data["access_token"]

def get_demographic_data(page_id, page_name, system_token, detailed=False):
    """H√§mta demografisk data f√∂r en sida med r√§tt perioder f√∂r varje metrik"""
    logger.info(f"H√§mtar demografisk data f√∂r sida: {page_name} (ID: {page_id})...")
    
    # F√∂rst h√§mta en Page Access Token f√∂r denna specifika sida
    page_token = get_page_access_token(page_id, system_token)
    
    if not page_token:
        logger.warning(f"‚ö†Ô∏è Kunde inte h√§mta Page Access Token f√∂r sida {page_name}")
        return {
            "page_id": page_id,
            "page_name": page_name,
            "error": "Kunde inte h√§mta page access token",
            "data": {}
        }
    
    # Skapa resultatstruktur
    result = {
        "page_id": page_id,
        "page_name": page_name,
        "error": None,
        "data": {}
    }
    
    # H√§mta varje demografisk metrik med r√§tt period
    has_any_data = False
    
    for metric_name, metric_info in DEMOGRAPHIC_METRICS.items():
        result["data"][metric_name] = {
            "display_name": metric_info["name"],
            "description": metric_info["description"],
            "values": {},
            "error": None
        }
        
        metric_success = False
        
        # Prova varje period som √§r giltig f√∂r denna metrik
        for period in metric_info["periods"]:
            if metric_success:
                break  # Hoppa √∂ver om vi redan har data
            
            url = f"https://graph.facebook.com/{API_VERSION}/{page_id}/insights"
            params = {
                "access_token": page_token,
                "metric": metric_name,
                "period": period
            }
            
            # F√∂r r√§ckviddmetriker, l√§gg till tidsgr√§nser f√∂r att f√• nyare data om det √§r en icke-lifetime period
            if period != "lifetime":
                # F√∂r nyare data, anv√§nd senaste 28 dagarna
                end_date = datetime.now()
                start_date = end_date - timedelta(days=28)
                params["since"] = start_date.strftime("%Y-%m-%d")
                params["until"] = end_date.strftime("%Y-%m-%d")
            
            logger.debug(f"H√§mtar {metric_name} f√∂r {page_name} med period={period}")
            
            try:
                data = api_request(url, params)
                
                # Analysera svaret med b√§ttre felhantering
                if data and "data" in data and data["data"]:
                    for metric_data in data["data"]:
                        if metric_data["name"] == metric_name:
                            values = metric_data.get("values", [])
                            
                            if values and len(values) > 0:
                                value_data = values[0].get("value", {})
                                
                                # Lagra v√§rdet i resultatet om det inte √§r tomt
                                if value_data and isinstance(value_data, dict) and len(value_data) > 0:
                                    result["data"][metric_name]["values"] = value_data
                                    result["data"][metric_name]["period"] = period
                                    logger.info(f"  ‚úì {metric_info['name']} data h√§mtad f√∂r {page_name} med period={period}")
                                    metric_success = True
                                    has_any_data = True
                                    break
                                elif value_data and not isinstance(value_data, dict):
                                    # Hantera icke-dictionary-v√§rden (t.ex. m√§tv√§rden som √§r nummer)
                                    result["data"][metric_name]["values"] = {"total": value_data}
                                    result["data"][metric_name]["period"] = period
                                    logger.info(f"  ‚úì {metric_info['name']} data h√§mtad f√∂r {page_name} med period={period}")
                                    metric_success = True
                                    has_any_data = True
                                    break
                
                # Om inget v√§rde hittades √§n
                if not metric_success and data and "error" in data:
                    error_msg = data["error"].get("message", "Ok√§nt fel")
                    error_code = data["error"].get("code", "")
                    
                    # Om det √§r ett felmeddelande om ogiltig metrik, lagra det men forts√§tt mjukt
                    if error_code == 100 and "valid insights metric" in error_msg:
                        logger.warning(f"  ‚úó Metrik {metric_name} √§r inte tillg√§nglig f√∂r denna sida, period={period}")
                        result["data"][metric_name]["error"] = f"Metrik inte tillg√§nglig: {error_msg}"
                    elif "insufficient" in error_msg.lower() or "permission" in error_msg.lower():
                        logger.warning(f"  ‚úó Otillr√§ckliga beh√∂righeter f√∂r {metric_name} med period={period}: {error_msg}")
                        result["data"][metric_name]["error"] = f"Otillr√§ckliga beh√∂righeter: {error_msg}"
                    else:
                        logger.warning(f"  ‚úó Fel vid h√§mtning av {metric_name} f√∂r sida {page_name}: {error_msg}")
                        result["data"][metric_name]["error"] = error_msg
                
            except Exception as e:
                logger.warning(f"  ‚úó Undantag vid h√§mtning av {metric_name} f√∂r sida {page_name}: {str(e)}")
                result["data"][metric_name]["error"] = str(e)
        
        # Om vi fortfarande inte har data f√∂r denna metrik, logga ett sammanfattande meddelande
        if not metric_success:
            logger.warning(f"  ‚úó Kunde inte h√§mta {metric_info['name']} f√∂r sida {page_name} med n√•gon period")
            # Endast uppdatera huvudfelet om vi inte redan har ett fel
            if result["error"] is None and result["data"][metric_name]["error"]:
                result["error"] = f"Fel f√∂r {metric_info['name']}: {result['data'][metric_name]['error']}"
    
    # Kontrollera om vi har n√•gon data alls
    if not has_any_data:
        if result["error"] is None:
            result["error"] = "Ingen demografisk data tillg√§nglig f√∂r denna sida"
        logger.warning(f"‚ö†Ô∏è Ingen demografisk data hittades f√∂r sida {page_name}")
    else:
        # Ber√§kna sammanfattande statistik
        total_fans_by_country = sum(result["data"].get("page_fans_country", {}).get("values", {}).values())
        total_fans_by_city = sum(result["data"].get("page_fans_city", {}).get("values", {}).values())
        
        logger.info(f"üìä Sammanfattning f√∂r {page_name}:")
        logger.info(f"  - Totalt antal fans fr√•n l√§nder: {total_fans_by_country:,}")
        logger.info(f"  - Totalt antal fans fr√•n st√§der: {total_fans_by_city:,}")
        
        # Om vi har k√∂ns- och √•ldersf√∂rdelning, visa sammanfattning
        gender_age_data = result["data"].get("page_fans_gender_age", {}).get("values", {})
        if gender_age_data:
            # Ber√§kna totaler per k√∂n
            gender_totals = {"M": 0, "F": 0, "U": 0}
            for key, value in gender_age_data.items():
                gender = key.split(".")[0]  # Format √§r "M.13-17", "F.18-24", etc.
                if gender in gender_totals:
                    gender_totals[gender] += value
            
            # Visa k√∂nf√∂rdelning
            total_with_gender = sum(gender_totals.values())
            if total_with_gender > 0:
                logger.info(f"  - K√∂nsf√∂rdelning:")
                male_percent = gender_totals["M"] / total_with_gender * 100 if total_with_gender > 0 else 0
                female_percent = gender_totals["F"] / total_with_gender * 100 if total_with_gender > 0 else 0
                unknown_percent = gender_totals["U"] / total_with_gender * 100 if total_with_gender > 0 else 0
                
                logger.info(f"    - M√§n: {gender_totals['M']:,} ({male_percent:.1f}%)")
                logger.info(f"    - Kvinnor: {gender_totals['F']:,} ({female_percent:.1f}%)")
                logger.info(f"    - Ok√§nt: {gender_totals['U']:,} ({unknown_percent:.1f}%)")
    
    # V√§nta lite mellan anrop f√∂r att inte √∂verlasta API:et
    time.sleep(1)
    
    return result

def process_pages(pages, output_file, selected_page_ids=None, detailed=False):
    """Bearbeta alla sidor och skapa en Excel-fil med demografisk data"""
    if selected_page_ids:
        # Filtrera endast valda sidor om en lista angetts
        filtered_pages = [(page_id, name, category, fans) for page_id, name, category, fans in pages 
                          if page_id in selected_page_ids]
        logger.info(f"Filtrerar till {len(filtered_pages)} av {len(pages)} sidor baserat p√• indata.")
        pages = filtered_pages
    
    logger.info(f"Bearbetar {len(pages)} sidor...")
    results = []
    
    for i, (page_id, page_name, category, fans) in enumerate(pages):
        logger.info(f"Bearbetar sida {i+1}/{len(pages)}: {page_name} ({category}, {fans:,} fans)")
        data = get_demographic_data(page_id, page_name, ACCESS_TOKEN, detailed=detailed)
        data["category"] = category
        data["fans"] = fans
        results.append(data)
        
        # V√§nta lite mellan anrop f√∂r att inte √∂verbelasta API:et
        if i < len(pages) - 1:
            time.sleep(2)
    
    # Skapa Excel-filen
    create_excel_report(results, output_file, detailed=detailed)
    
    return results

def format_excel_sheet(worksheet, title, start_row=0):
    """St√§ll in grundl√§ggande formatering f√∂r ett Excel-ark"""
    # St√§ll in titeln
    title_cell = worksheet.cell(row=start_row+1, column=1)
    title_cell.value = title
    title_cell.font = Font(bold=True, size=14)
    worksheet.merge_cells(start_row=start_row+1, start_column=1, end_row=start_row+1, end_column=5)
    title_cell.alignment = Alignment(horizontal='center')
    
    # St√§ll in kolumnbredderna
    worksheet.column_dimensions['A'].width = 30
    worksheet.column_dimensions['B'].width = 15
    worksheet.column_dimensions['C'].width = 15
    
    # Formatera rubriker
    header_fill = PatternFill(start_color="DDEBF7", end_color="DDEBF7", fill_type="solid")
    header_font = Font(bold=True)
    thin_border = Border(
        left=Side(style='thin'), 
        right=Side(style='thin'), 
        top=Side(style='thin'), 
        bottom=Side(style='thin')
    )
    
    for row in worksheet.iter_rows(min_row=start_row+3, max_row=start_row+3, min_col=1, max_col=5):
        for cell in row:
            cell.fill = header_fill
            cell.font = header_font
            cell.border = thin_border
            cell.alignment = Alignment(horizontal='center')
    
    return worksheet

def create_excel_report(results, output_file, detailed=False):
    """Skapa en Excel-rapport med en flik per sida"""
    logger.info(f"Skapar Excel-rapport: {output_file}")
    
    # Skapa en Excel-writer
    writer = pd.ExcelWriter(output_file, engine='openpyxl')
    
    # Kontrollera att utdatakatalogen finns
    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)
    
    # Skapa √§ven CSV-export f√∂r varje datatyp
    csv_dir = os.path.join(os.path.dirname(output_file), "csv_export")
    if not os.path.exists(csv_dir):
        os.makedirs(csv_dir)
    
    # Samla all data f√∂r CSV-export
    all_data_rows = []
    
    # Skapa √∂versiktsflik
    overview_data = []
    for result in results:
        # Sammanst√§ll statistik
        page_name = result["page_name"]
        page_id = result["page_id"]
        fans = result.get("fans", 0)
        category = result.get("category", "Ok√§nd")
        
        # R√§kna antalet objekt f√∂r varje metrik
        city_fans_count = len(result["data"].get("page_fans_city", {}).get("values", {}))
        country_fans_count = len(result["data"].get("page_fans_country", {}).get("values", {}))
        gender_age_fans_count = len(result["data"].get("page_fans_gender_age", {}).get("values", {}))
        locale_fans_count = len(result["data"].get("page_fans_locale", {}).get("values", {}))
        
        # Ber√§kna totalsumma av fans fr√•n olika m√•ttv√§rden n√§r det √§r tillg√§ngligt
        total_country_fans = sum(result["data"].get("page_fans_country", {}).get("values", {}).values())
        total_city_fans = sum(result["data"].get("page_fans_city", {}).get("values", {}).values())
        total_gender_age_fans = sum(result["data"].get("page_fans_gender_age", {}).get("values", {}).values())
        
        # Sammanst√§ll k√∂nf√∂rdelning om tillg√§nglig
        gender_age_data = result["data"].get("page_fans_gender_age", {}).get("values", {})
        male_fans = sum(v for k, v in gender_age_data.items() if k.startswith("M."))
        female_fans = sum(v for k, v in gender_age_data.items() if k.startswith("F."))
        unknown_gender_fans = sum(v for k, v in gender_age_data.items() if k.startswith("U."))
        
        # Ber√§kna procent av k√∂nsf√∂rdelning
        total_gender = male_fans + female_fans + unknown_gender_fans
        male_percent = (male_fans / total_gender * 100) if total_gender > 0 else 0
        female_percent = (female_fans / total_gender * 100) if total_gender > 0 else 0
        unknown_percent = (unknown_gender_fans / total_gender * 100) if total_gender > 0 else 0
        
        # Skapa √∂versiktsraden
        page_data = {
            "Sida": page_name,
            "ID": page_id,
            "Kategori": category,
            "Deklarerade fans": fans,
            "API Fans (l√§nder)": total_country_fans,
            "API Fans (st√§der)": total_city_fans,
            "API Fans (k√∂n/√•lder)": total_gender_age_fans,
            "M√§n": male_fans,
            "M√§n %": f"{male_percent:.1f}%",
            "Kvinnor": female_fans,
            "Kvinnor %": f"{female_percent:.1f}%",
            "Ok√§nt k√∂n": unknown_gender_fans,
            "Ok√§nt k√∂n %": f"{unknown_percent:.1f}%",
            "Antal l√§nder": country_fans_count,
            "Antal st√§der": city_fans_count,
            "Antal √•ldersgrupper": gender_age_fans_count,
            "Antal spr√•k": locale_fans_count,
            "Status": "OK" if result["error"] is None else "FEL",
            "Felmeddelande": result["error"] or ""
        }
        overview_data.append(page_data)
    
    # Skapa √∂versiktsfliken
    if overview_data:
        overview_df = pd.DataFrame(overview_data)
        overview_df.to_excel(writer, sheet_name="√ñversikt", index=False)
        
        # Formatera √∂versiktsfliken
        worksheet = writer.sheets["√ñversikt"]
        for column in overview_df:
            column_width = max(
                overview_df[column].astype(str).map(len).max(), 
                len(column)
            )
            col_idx = overview_df.columns.get_loc(column) + 1
            worksheet.column_dimensions[get_column_letter(col_idx)].width = min(column_width + 2, 30)
    
    # Skapa en flik per sida med detaljerad data
    for result in results:
        page_name = result["page_name"]
        page_id = result["page_id"]
        sheet_name = page_name[:31]  # Excel har en begr√§nsning p√• 31 tecken f√∂r fliknamn
        
        # Om vi har ett dupliktflknamn, l√§gg till en del av ID:t
        suffix = 1
        original_sheet_name = sheet_name
        while sheet_name in writer.sheets:
            sheet_name = f"{original_sheet_name[:27]}_{suffix}"
            suffix += 1
        
        # Skapa ett DataFrame f√∂r sidans basinfo
        info_df = pd.DataFrame({
            "Information": [
                "Sidnamn", 
                "Sid-ID", 
                "Kategori", 
                "Antal fans (deklarerat)", 
                "Status", 
                "Felmeddelande", 
                "Datum f√∂r rapport"
            ],
            "V√§rde": [
                result["page_name"], 
                result["page_id"], 
                result.get("category", "Ok√§nd"),
                result.get("fans", 0),
                "OK" if result["error"] is None else "FEL",
                result["error"] or "",
                datetime.now().strftime("%Y-%m-%d %H:%M")
            ]
        })
        
        # Skriv basinfo till Excel-flik
        info_df.to_excel(writer, sheet_name=sheet_name, index=False, startrow=0, startcol=0)
        
        row_offset = 9  # B√∂rja efter infon
        
        # Organisera data i grupper
        data_groups = [
            {"title": "Fans per land", "metric": "page_fans_country", "columns": ["Land", "Antal fans"]},
            {"title": "Fans per stad", "metric": "page_fans_city", "columns": ["Stad", "Antal fans"]},
            {"title": "Fans per k√∂n och √•lder", "metric": "page_fans_gender_age", "columns": ["K√∂n och √•lder", "Antal fans"]},
            {"title": "Fans per spr√•k", "metric": "page_fans_locale", "columns": ["Spr√•k", "Antal fans"]},
            {"title": "Fans online per veckodag", "metric": "page_fans_online_per_day", "columns": ["Veckodag", "Antal fans"]},
            {"title": "Fans online per timme", "metric": "page_fans_online", "columns": ["Timme", "Antal fans"]},
            {"title": "R√§ckvidd per land", "metric": "page_impressions_by_country_unique", "columns": ["Land", "R√§ckvidd"]},
            {"title": "R√§ckvidd per stad", "metric": "page_impressions_by_city_unique", "columns": ["Stad", "R√§ckvidd"]},
            {"title": "R√§ckvidd per k√∂n och √•lder", "metric": "page_impressions_by_age_gender_unique", "columns": ["K√∂n och √•lder", "R√§ckvidd"]},
            {"title": "R√§ckvidd per webbl√§sare", "metric": "page_impressions_by_browser_unique", "columns": ["Webbl√§sare", "R√§ckvidd"]},
            {"title": "R√§ckvidd per enhetstyp", "metric": "page_impressions_by_device_type_unique", "columns": ["Enhetstyp", "R√§ckvidd"]}
        ]
        
        for group in data_groups:
            # H√§mta data f√∂r denna metrik
            metric_data = result["data"].get(group["metric"], {})
            metric_values = metric_data.get("values", {})
            metric_error = metric_data.get("error", None)
            metric_period = metric_data.get("period", "")
            
            # Skapa titel
            title = f"{group['title']} ({metric_period})" if metric_period else group["title"]
            title_df = pd.DataFrame([[title]])
            title_df.to_excel(writer, sheet_name=sheet_name, startrow=row_offset, startcol=0, header=False, index=False)
            row_offset += 1
            
            if metric_values:
                # Skapa dataframe och sortera efter v√§rde (h√∂gst f√∂rst)
                df = pd.DataFrame(metric_values.items(), columns=group["columns"])
                
                # F√∂r gender_age, konvertera till mer l√§sbara etiketter
                if group["metric"] == "page_fans_gender_age" or group["metric"] == "page_impressions_by_age_gender_unique":
                    df[group["columns"][0]] = df[group["columns"][0]].apply(lambda x: format_gender_age(x))
                
                # Sortera efter v√§rde h√∂gst f√∂rst
                df = df.sort_values(group["columns"][1], ascending=False)
                
                # Skriv data
                df.to_excel(writer, sheet_name=sheet_name, startrow=row_offset, startcol=0, index=False)
                
                # Ber√§kna totalsumma
                total = sum(metric_values.values())
                total_df = pd.DataFrame([["TOTALT", total]], columns=group["columns"])
                total_df.to_excel(writer, sheet_name=sheet_name, startrow=row_offset+len(df)+1, startcol=0, header=False, index=False)
                
                # Uppdatera rad-offset f√∂r n√§sta grupp
                row_offset += len(df) + 4
                
                # L√§gg till data f√∂r CSV-export
                for key, value in metric_values.items():
                    formatted_key = format_gender_age(key) if group["metric"] in ["page_fans_gender_age", "page_impressions_by_age_gender_unique"] else key
                    all_data_rows.append({
                        "Page name": page_name,
                        "Page ID": page_id,
                        "Dimension": group["metric"],
                        "Category": group["title"],
                        "Key": formatted_key,
                        "Value": value,
                        "Period": metric_period
                    })
            else:
                # Skapa "ingen data" meddelande
                no_data_message = f"Ingen data tillg√§nglig: {metric_error}" if metric_error else "Ingen data tillg√§nglig"
                no_data_df = pd.DataFrame([[no_data_message]])
                no_data_df.to_excel(writer, sheet_name=sheet_name, startrow=row_offset, startcol=0, header=False, index=False)
                
                row_offset += 3
            
            # L√§gg till lite extra rader mellan grupperna
            row_offset += 1
        
        logger.debug(f"Skapade flik f√∂r {page_name}")
    
    # Spara Excel-filen
    writer.close()
    logger.info(f"‚úÖ Excel-rapport sparad till {output_file}")
    
    # Skapa CSV-export
    if all_data_rows:
        csv_path = os.path.join(csv_dir, "demographic_full_export.csv")
        csv_df = pd.DataFrame(all_data_rows)
        csv_df.to_csv(csv_path, index=False, encoding='utf-8')
        logger.info(f"‚úÖ CSV-export sparad till {csv_path}")
        
        # Skapa specifika exports per dimensionstyp
        for dimension in set(row["Dimension"] for row in all_data_rows):
            dimension_data = [row for row in all_data_rows if row["Dimension"] == dimension]
            if dimension_data:
                clean_dimension = dimension.replace("page_", "").replace("_", "-")
                dimension_csv_path = os.path.join(csv_dir, f"demographic_{clean_dimension}.csv")
                dimension_df = pd.DataFrame(dimension_data)
                dimension_df.to_csv(dimension_csv_path, index=False, encoding='utf-8')
                logger.info(f"‚úÖ CSV f√∂r {clean_dimension} sparad till {dimension_csv_path}")

def format_gender_age(key):
    """Formatera k√∂n och √•ldersnycklar till l√§sbara etiketter"""
    if not isinstance(key, str) or "." not in key:
        return key
        
    gender, age_range = key.split(".", 1)
    
    gender_map = {
        "M": "Man",
        "F": "Kvinna",
        "U": "Ok√§nt k√∂n"
    }
    
    gender_text = gender_map.get(gender, gender)
    return f"{gender_text}, {age_range}"

def parse_args():
    """Parsa kommandoradsargument"""
    parser = argparse.ArgumentParser(description="H√§mta demografisk data f√∂r Facebook-sidor")
    parser.add_argument("--output", help="Filnamn f√∂r Excel-rapporten (standard: fb_demographics.xlsx)", default="fb_demographics.xlsx")
    parser.add_argument("--pages", help="Kommaseparerad lista med sida-ID:n (om tom, h√§mtas alla tillg√§ngliga sidor)")
    parser.add_argument("--detailed", action="store_true", help="H√§mta detaljerad data (tar l√§ngre tid)")
    parser.add_argument("--debug", action="store_true", help="Aktivera utf√∂rlig loggning")
    return parser.parse_args()

def main():
    """Huvudfunktion"""
    args = parse_args()
    
    # S√§tt debug-l√§ge om beg√§rt
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.debug("Debug-l√§ge aktiverat")
    
    logger.info(f"üìä Facebook Demographics Reporter ‚Äì v2.0")
    logger.info(f"Output-fil: {args.output}")
    logger.info("-------------------------------------------------------------------")
    
    # Kontrollera token och varna om den snart g√•r ut
    check_token_expiry()
    
    # Validera token
    if not validate_token(ACCESS_TOKEN):
        logger.error("‚ùå Token kunde inte valideras. Avbryter.")
        return
    
    # Lista med specifika page IDs om angivna
    selected_page_ids = None
    if args.pages:
        selected_page_ids = [page_id.strip() for page_id in args.pages.split(",")]
        logger.info(f"Kommer endast att h√§mta data f√∂r {len(selected_page_ids)} specifika sidor")
    
    # H√§mta sidor att bearbeta
    page_list = get_page_ids_with_access(ACCESS_TOKEN)
    
    if not page_list:
        logger.error("‚ùå Inga sidor hittades. Avbryter.")
        return
    
    # Bearbeta sidor och skapa rapport
    results = process_pages(page_list, args.output, selected_page_ids=selected_page_ids, detailed=args.detailed)
    
    # Visa statistik om API-anv√§ndning
    elapsed_time = time.time() - start_time
    logger.info(f"‚è±Ô∏è Total k√∂rtid: {elapsed_time:.1f} sekunder")
    logger.info(f"üåê API-anrop: {api_call_count} ({api_call_count/elapsed_time*3600:.1f}/timme)")
    logger.info(f"‚úÖ Klar! Bearbetade {len(results)} sidor")
    
    # Visa sammanfattade resultat
    successful_pages = sum(1 for r in results if r["error"] is None)
    logger.info(f"üìä Lyckades h√§mta data f√∂r {successful_pages} av {len(results)} sidor")
    
    # Visa n√•gra sammanfattande felorsaker om relevanta
    error_types = {}
    for r in results:
        if r["error"]:
            # Extrahera grundorsak fr√•n felmeddelandet
            error_type = "Annan fel"
            if "not available" in r["error"].lower() or "valid insights metric" in r["error"].lower():
                error_type = "Metrik ej tillg√§nglig"
            elif "permission" in r["error"].lower():
                error_type = "Beh√∂righetsfel"
            elif "token" in r["error"].lower():
                error_type = "Token-problem"
            elif "rate limit" in r["error"].lower():
                error_type = "Rate limit"
            
            error_types[error_type] = error_types.get(error_type, 0) + 1
    
    if error_types:
        logger.info("Vanliga felorsaker:")
        for error_type, count in error_types.items():
            logger.info(f"  - {error_type}: {count} sidor")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("Avbruten av anv√§ndare.")
        sys.exit(1)
    except Exception as e:
        logger.critical(f"Ov√§ntat fel: {e}")
        import traceback
        logger.critical(traceback.format_exc())
        sys.exit(1)
