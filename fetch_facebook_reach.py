# fetch_facebook_reach.py

import csv
import json
import os
import time
import requests
import logging
import argparse
import sys
import glob
import pandas as pd
from datetime import datetime, timedelta
from calendar import monthrange
from config import (
    ACCESS_TOKEN, TOKEN_LAST_UPDATED, INITIAL_START_YEAR_MONTH,
    API_VERSION, CACHE_FILE, 
    BATCH_SIZE, MAX_RETRIES, RETRY_DELAY, 
    TOKEN_VALID_DAYS, MAX_REQUESTS_PER_HOUR,
    MONTH_PAUSE_SECONDS
)

# Skapa datumst√§mplad loggfil
def setup_logging():
    """Konfigurera loggning med datumst√§mplad loggfil"""
    now = datetime.now()
    log_dir = "logs"
    
    # Skapa loggdirektory om den inte finns
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    # Skapa datumst√§mplad loggfilnamn
    log_filename = os.path.join(log_dir, f"facebook_reach_{now.strftime('%Y-%m-%d_%H-%M-%S')}.log")
    
    # Konfigurera loggning
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename),  # Datumst√§mplad loggfil
            logging.FileHandler("facebook_reach.log"),  # Beh√•ll den senaste loggfilen f√∂r enkelt √•tkomst
            logging.StreamHandler()  # Terminal-utskrift
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"Startar loggning till fil: {log_filename}")
    
    return logger

# Konfigurera loggning med datumst√§mplad fil
logger = setup_logging()

# R√§knare f√∂r API-anrop
api_call_count = 0
start_time = time.time()

def check_token_expiry():
    """Kontrollera om token snart g√•r ut och varna anv√§ndaren"""
    try:
        last_updated = datetime.strptime(TOKEN_LAST_UPDATED, "%Y-%m-%d")
        days_since = (datetime.now() - last_updated).days
        days_left = TOKEN_VALID_DAYS - days_since
        
        logger.info(f"üîë Token skapades f√∂r {days_since} dagar sedan ({days_left} dagar kvar till utg√•ng).")
        
        if days_left <= 7:
            logger.warning(f"‚ö†Ô∏è VARNING: Din token g√•r ut inom {days_left} dagar! Skapa en ny token snart.")
        elif days_left <= 0:
            logger.error(f"‚ùå KRITISKT: Din token har g√•tt ut! Skapa en ny token omedelbart.")
            sys.exit(1)
    except Exception as e:
        logger.error(f"‚ö†Ô∏è Kunde inte tolka TOKEN_LAST_UPDATED: {e}")

def load_page_cache():
    """Ladda cache med sidnamn f√∂r att minska API-anrop"""
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                logger.debug(f"Laddar sid-cache fr√•n {CACHE_FILE}")
                return json.load(f)
        except json.JSONDecodeError:
            logger.warning(f"Kunde inte ladda cache-fil, skapar ny cache")
    return {}

def save_page_cache(cache):
    """Spara cache med sidnamn f√∂r framtida k√∂rningar"""
    try:
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
            logger.debug(f"Sparade sid-cache till {CACHE_FILE}")
    except Exception as e:
        logger.error(f"Kunde inte spara cache: {e}")

def api_request(url, params, retries=MAX_RETRIES):
    """G√∂r API-f√∂rfr√•gan med √•terf√∂rs√∂k och rate limit-hantering"""
    global api_call_count
    
    # Kontrollera om vi n√§rmar oss rate limit
    current_time = time.time()
    elapsed_hours = (current_time - start_time) / 3600
    rate = api_call_count / elapsed_hours if elapsed_hours > 0 else 0
    
    if rate > MAX_REQUESTS_PER_HOUR * 0.9:  # Om vi anv√§nt 90% av rate limit
        wait_time = 3600 / MAX_REQUESTS_PER_HOUR  # V√§nta tillr√§ckligt f√∂r att h√•lla oss under gr√§nsen
        logger.warning(f"N√§rmar oss rate limit ({int(rate)}/h). V√§ntar {wait_time:.1f} sekunder...")
        time.sleep(wait_time)
    
    for attempt in range(retries):
        try:
            api_call_count += 1
            response = requests.get(url, params=params, timeout=30)
            
            # Hantera vanliga HTTP-fel
            if response.status_code == 429:  # Too Many Requests
                retry_after = int(response.headers.get('Retry-After', RETRY_DELAY))
                logger.warning(f"Rate limit n√•tt! V√§ntar {retry_after} sekunder... (f√∂rs√∂k {attempt+1}/{retries})")
                time.sleep(retry_after)
                continue
                
            elif response.status_code >= 500:  # Server error
                wait_time = RETRY_DELAY * (2 ** attempt)  # Exponentiell backoff
                logger.warning(f"Serverfel: {response.status_code}. V√§ntar {wait_time} sekunder... (f√∂rs√∂k {attempt+1}/{retries})")
                time.sleep(wait_time)
                continue
            
            # F√∂r alla HTTP-svarkoder, f√∂rs√∂k tolka JSON-inneh√•llet
            try:
                json_data = response.json()
                
                # S√§rskild hantering f√∂r 400-fel (Bad Request)
                if response.status_code == 400 and "error" in json_data:
                    error_code = json_data["error"].get("code")
                    error_msg = json_data["error"].get("message", "Ok√§nt fel")
                    
                    # Hantera specifika felkoder
                    if error_code == 4:  # App-specifikt rate limit
                        wait_time = 60 * (attempt + 1)  # V√§nta l√§ngre f√∂r varje f√∂rs√∂k
                        logger.warning(f"App rate limit: {error_msg}. V√§ntar {wait_time} sekunder...")
                        time.sleep(wait_time)
                        continue
                        
                    elif error_code == 190:  # Ogiltig token
                        logger.error(f"Access token ogiltig: {error_msg}")
                        return None
                
                # Om vi kommer hit och har en icke-200 status, logga felet men returnera √§nd√• JSON-data
                # s√• att anropande funktion kan hantera felet mer detaljerat
                if response.status_code != 200:
                    logger.error(f"HTTP-fel {response.status_code}: {response.text}")
                    
                    if attempt < retries - 1:
                        wait_time = RETRY_DELAY * (2 ** attempt)
                        logger.info(f"V√§ntar {wait_time} sekunder innan nytt f√∂rs√∂k... (f√∂rs√∂k {attempt+1}/{retries})")
                        time.sleep(wait_time)
                        continue
                    
                    # Returnera √§nd√• JSON-data s√• att anropande funktion kan hantera felet
                    return json_data
                
                # Allt gick bra, returnera data
                return json_data
                
            except json.JSONDecodeError:
                logger.error(f"Kunde inte tolka JSON-svar: {response.text[:100]}")
                if attempt < retries - 1:
                    wait_time = RETRY_DELAY * (2 ** attempt)
                    logger.info(f"V√§ntar {wait_time} sekunder innan nytt f√∂rs√∂k... (f√∂rs√∂k {attempt+1}/{retries})")
                    time.sleep(wait_time)
                    continue
                return None
                
        except requests.RequestException as e:
            logger.error(f"N√§tverksfel: {e}")
            if attempt < retries - 1:
                wait_time = RETRY_DELAY * (2 ** attempt)
                logger.info(f"V√§ntar {wait_time} sekunder innan nytt f√∂rs√∂k... (f√∂rs√∂k {attempt+1}/{retries})")
                time.sleep(wait_time)
            else:
                return None
    
    return None

def validate_token(token):
    """Validera att token √§r giltig och h√§mta anv√§ndarbeh√∂righeter"""
    logger.info("Validerar token...")
    url = f"https://graph.facebook.com/{API_VERSION}/debug_token"
    params = {"input_token": token, "access_token": token}
    
    data = api_request(url, params)
    
    if not data or "data" not in data:
        logger.error("‚ùå Kunde inte validera token")
        return False
        
    if not data["data"].get("is_valid"):
        logger.error(f"‚ùå Token √§r ogiltig: {data['data'].get('error', {}).get('message', 'Ok√§nd anledning')}")
        return False
        
    logger.info(f"‚úÖ Token validerad. App ID: {data['data'].get('app_id')}")
    return True

def get_page_ids_with_access(token):
    """H√§mta alla sidor som token har √•tkomst till"""
    logger.info("H√§mtar tillg√§ngliga sidor...")
    url = f"https://graph.facebook.com/{API_VERSION}/me/accounts"
    params = {"access_token": token, "limit": 100, "fields": "id,name"}
    
    pages = []
    next_url = url
    
    while next_url:
        data = api_request(url if next_url == url else next_url, {} if next_url != url else params)
        
        if not data or "data" not in data:
            break
            
        pages.extend(data["data"])
        logger.debug(f"Hittade {len(data['data'])} sidor i denna batch")
        
        # Hantera paginering
        next_url = data.get("paging", {}).get("next")
        if next_url and next_url != url:
            logger.debug(f"H√§mtar n√§sta sida fr√•n: {next_url}")
        else:
            break
    
    if not pages:
        logger.warning("Inga sidor hittades. Token kanske saknar 'pages_show_list'-beh√∂righet.")
    
    page_ids = [(page["id"], page["name"]) for page in pages]
    logger.info(f"‚úÖ Hittade {len(page_ids)} sidor att analysera")
    return page_ids

def get_page_name(page_id, cache):
    """H√§mta sidans namn fr√•n cache eller API"""
    if page_id in cache:
        return cache[page_id]
    
    logger.debug(f"H√§mtar namn f√∂r sida {page_id}...")
    url = f"https://graph.facebook.com/{API_VERSION}/{page_id}"
    params = {"fields": "name", "access_token": ACCESS_TOKEN}
    
    data = api_request(url, params)
    
    if not data or "error" in data:
        error_msg = data.get("error", {}).get("message", "Ok√§nt fel") if data else "Fel vid API-anrop"
        logger.warning(f"‚ö†Ô∏è Kunde inte h√§mta namn f√∂r sida {page_id}: {error_msg}")
        return None
    
    name = data.get("name", f"Page {page_id}")
    cache[page_id] = name
    return name

def get_page_access_token(page_id, system_token):
    """Konvertera systemanv√§ndartoken till en Page Access Token f√∂r en specifik sida"""
    logger.debug(f"H√§mtar Page Access Token f√∂r sida {page_id}...")
    url = f"https://graph.facebook.com/{API_VERSION}/{page_id}"
    params = {
        "fields": "access_token",
        "access_token": system_token
    }
    
    data = api_request(url, params)
    
    if not data or "error" in data or "access_token" not in data:
        error_msg = data.get("error", {}).get("message", "Ok√§nt fel") if data and "error" in data else "Kunde inte h√§mta token"
        logger.warning(f"‚ö†Ô∏è Kunde inte h√§mta Page Access Token f√∂r sida {page_id}: {error_msg}")
        return None
    
    return data["access_token"]

def get_page_metrics(page_id, system_token, since, until, page_name=None):
    """H√§mta r√§ckvidd och interaktionsdata f√∂r en sida under en specifik tidsperiod"""
    display_name = page_name if page_name else page_id
    logger.debug(f"H√§mtar metriker f√∂r sida {display_name} fr√•n {since} till {until}...")
    
    # Skapa resultatstruktur
    result = {
        "reach": 0,
        "engaged_users": 0,
        "engagements": 0,
        "reactions": 0,
        "reactions_details": {},  # Lagra detaljerade reaktionsdata
        "status": "OK",           # Defaultstatus
        "comment": ""             # Plats f√∂r ytterligare information om felet
    }
    
    # F√∂rst h√§mta en Page Access Token f√∂r denna specifika sida
    page_token = get_page_access_token(page_id, system_token)
    
    if not page_token:
        result["status"] = "NO_ACCESS"
        result["comment"] = "Kunde inte h√§mta Page Access Token"
        logger.warning(f"‚ö†Ô∏è Kunde inte h√§mta Page Access Token f√∂r sida {display_name}")
        return result
    
    # Definition av metriker och deras mappning
    metrics_mapping = [
        {"api_name": "page_impressions_unique", "result_key": "reach", "display_name": "R√§ckvidd"},
        {"api_name": "page_post_engagements", "result_key": "engagements", "display_name": "Interaktioner"},
        {"api_name": "page_actions_post_reactions_total", "result_key": "reactions", "display_name": "Reaktioner"}
    ]
    
    api_errors = []  # Samla fel fr√•n API-anrop
    
    # H√§mta varje metrik separat f√∂r att isolera fel
    for metric_info in metrics_mapping:
        try:
            # Anv√§nd Page Access Token f√∂r att h√§mta insikter
            url = f"https://graph.facebook.com/{API_VERSION}/{page_id}/insights"
            params = {
                "access_token": page_token,
                "since": since,
                "until": until,
                "period": "total_over_range",
                "metric": metric_info["api_name"]
            }
            
            data = api_request(url, params)
            
            if data and "data" in data and data["data"]:
                # Extrahera v√§rden fr√•n svaret
                for metric in data["data"]:
                    if metric["values"] and len(metric["values"]) > 0:
                        value = metric["values"][0].get("value", 0)
                        
                        # S√§rskild hantering f√∂r reaktioner som kan vara dictionary
                        if metric_info["result_key"] == "reactions" and isinstance(value, dict):
                            # Spara detaljerade reaktionsdata
                            result["reactions_details"] = value
                            # Ber√§kna summan av alla reaktioner
                            total_reactions = sum(int(v) for k, v in value.items() 
                                              if isinstance(v, (int, float)) or 
                                              (isinstance(v, str) and v.isdigit()))
                            
                            logger.info(f"Reaktioner f√∂r {display_name}: {value}, totalt: {total_reactions}")
                            result[metric_info["result_key"]] = total_reactions
                        else:
                            result[metric_info["result_key"]] = value
                            
                        logger.debug(f"  ‚úì {metric_info['display_name']} f√∂r {display_name}: {value}")
            elif data and "error" in data:
                # H√§r f√•ngar vi upp och ger ett tydligt felmeddelande per metrik
                error_msg = data["error"].get("message", "Ok√§nt fel")
                error_code = data["error"].get("code", "N/A")
                api_errors.append(f"{metric_info['display_name']}: {error_msg} (kod {error_code})")
                logger.error(f"Error {error_code}: Saknas m√§tv√§rde '{metric_info['display_name']}' f√∂r sida '{display_name}': {error_msg}")
            else:
                logger.warning(f"  ‚úó Kunde inte h√§mta {metric_info['display_name']} f√∂r sida {display_name}: Inget data")
                
        except Exception as e:
            # Logga felet f√∂r denna specifika metrik
            api_errors.append(f"{metric_info['display_name']}: {str(e)}")
            logger.warning(f"  ‚úó Fel vid h√§mtning av {metric_info['display_name']} f√∂r sida {display_name}: {e}")
            continue
    
    # Kontrollera och uppdatera status baserat p√• resultatet
    if api_errors:
        result["status"] = "API_ERROR"
        result["comment"] = "; ".join(api_errors[:3])  # Begr√§nsa l√§ngden p√• kommentaren
    elif all(result[key] == 0 for key in ["reach", "engaged_users", "engagements", "reactions"]):
        result["status"] = "NO_DATA"
        result["comment"] = "Alla v√§rden √§r noll"
    
    # Returnera resultatet oavsett status
    return result

def read_existing_csv(filename):
    """L√§s in befintlig CSV-fil och returnera en dict med Page ID -> data"""
    existing_data = {}
    if os.path.exists(filename):
        try:
            with open(filename, mode="r", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if "Page ID" in row:
                        # Konvertera numeriska v√§rden till heltal
                        page_data = {
                            "Page": row["Page"],
                            "Page ID": row["Page ID"],
                            "Reach": int(row.get("Reach", 0))
                        }
                        
                        # Hantera nya interaktionsf√§lt om de finns
                        if "Engaged Users" in row:
                            page_data["Engaged Users"] = int(row.get("Engaged Users", 0))
                        if "Engagements" in row:
                            page_data["Engagements"] = int(row.get("Engagements", 0))
                        if "Reactions" in row:
                            # Konvertera Reactions till heltal om m√∂jligt
                            try:
                                page_data["Reactions"] = int(row.get("Reactions", 0))
                            except ValueError:
                                # Om det √§r ett dictionary eller annat format som inte kan konverteras
                                page_data["Reactions"] = 0
                        
                        # Hantera statusf√§lt om det finns
                        if "Status" in row:
                            page_data["Status"] = row["Status"]
                        if "Comment" in row:
                            page_data["Comment"] = row["Comment"]
                            
                        existing_data[row["Page ID"]] = page_data
            logger.info(f"L√§ste in {len(existing_data)} befintliga sidor fr√•n {filename}")
        except Exception as e:
            logger.error(f"Fel vid inl√§sning av befintlig CSV-fil {filename}: {e}")
    return existing_data

def process_in_batches(page_list, cache, start_date, end_date, existing_data=None, batch_size=BATCH_SIZE):
    """Bearbeta sidor i batches f√∂r att f√∂rb√§ttra prestanda"""
    total_pages = len(page_list)
    results = []
    success = 0
    failed = 0
    skipped = 0
    
    # Om vi har befintlig data, l√§gg till den i resultatlistan f√∂rst
    if existing_data:
        # F√∂r varje existerande sida, s√§tt statusen till SKIPPED
        for page_id, data in existing_data.items():
            data["Status"] = "SKIPPED"
            data["Comment"] = "Redan i CSV-filen"
        results = list(existing_data.values())
        
    # Skapa en upps√§ttning av sidor som redan finns i befintlig data
    existing_page_ids = set(existing_data.keys()) if existing_data else set()
    
    for i in range(0, total_pages, batch_size):
        batch = page_list[i:i+batch_size]
        logger.info(f"Bearbetar batch {i//batch_size + 1}/{(total_pages + batch_size - 1)//batch_size} ({len(batch)} sidor)")
        
        for page_id, page_name in batch:
            # Hoppa √∂ver sidor som redan finns i den befintliga datan
            if page_id in existing_page_ids:
                logger.debug(f"Hoppar √∂ver sida {page_id} ({page_name}) eftersom den redan finns i CSV-filen")
                skipped += 1
                continue
                
            try:
                # Anv√§nd det redan k√§nda namnet om det finns
                name = page_name or get_page_name(page_id, cache)
                
                if not name:
                    logger.warning(f"‚ö†Ô∏è Kunde inte hitta namn f√∂r sida {page_id}, hoppar √∂ver")
                    failed += 1
                    continue
                
                logger.info(f"üìä H√§mtar data f√∂r: {name} (ID: {page_id}) [#{i + batch.index((page_id, page_name)) + 1}/{total_pages}]")
                metrics = get_page_metrics(page_id, ACCESS_TOKEN, start_date, end_date, page_name=name)
                
                if metrics is not None:
                    # Skapa basresultat med grundl√§ggande metriker
                    page_result = {
                        "Page": name,
                        "Page ID": page_id,
                        "Reach": metrics["reach"],
                        "Engaged Users": metrics["engaged_users"],
                        "Engagements": metrics["engagements"],
                        "Reactions": metrics["reactions"],
                        "Status": metrics["status"],            # L√§gg till status i resultatet
                        "Comment": metrics.get("comment", "")   # L√§gg till eventuell kommentar
                    }
                    
                    # L√§gg till detaljerade reaktioner om de finns
                    if "reactions_details" in metrics and metrics["reactions_details"]:
                        # Logga detaljerade reaktioner men inkludera dem inte i resultatet (kan g√∂ras om √∂nskat)
                        reactions_details = metrics["reactions_details"]
                        logger.info(f"Detaljerade reaktioner f√∂r {name}: {reactions_details}")
                    
                    results.append(page_result)
                    success += 1
                else:
                    logger.warning(f"‚ö†Ô∏è Inga data f√∂r sida {page_id} ({name})")
                    results.append({
                        "Page": name,
                        "Page ID": page_id,
                        "Reach": 0,
                        "Engaged Users": 0,
                        "Engagements": 0,
                        "Reactions": 0,
                        "Status": "UNKNOWN",
                        "Comment": "Ov√§ntat fel vid h√§mtning av data"
                    })
                    failed += 1
            except Exception as e:
                logger.error(f"Fel vid bearbetning av sida {page_id}: {e}")
                failed += 1
        
        # Visa framsteg
        total_processed = success + failed + skipped
        progress = total_processed / (total_pages + len(existing_page_ids)) * 100
        logger.info(f"Framsteg: {progress:.1f}% klar ({success} lyckade, {failed} misslyckade)")
        
        # Spara cache regelbundet f√∂r att inte f√∂rlora data vid fel
        if i % (batch_size * 5) == 0 and i > 0:
            save_page_cache(cache)
    
    return results, success, failed, skipped

def safe_int_value(value, default=0):
    """S√§kerst√§ller att ett v√§rde √§r ett heltal, och hanterar olika datatyper"""
    if isinstance(value, (int, float)):
        return int(value)
    elif isinstance(value, str) and value.strip().isdigit():
        return int(value)
    elif isinstance(value, dict):
        # Om det √§r ett dictionary med reaktioner, summera alla v√§rden
        try:
            # Filtrera ut eventuella icke-numeriska v√§rden
            total = sum(int(v) for k, v in value.items() if isinstance(v, (int, float)) or (isinstance(v, str) and v.isdigit()))
            logger.info(f"Summerar reaktioner fr√•n dictionary: {value} = {total}")
            return total
        except Exception as e:
            logger.warning(f"Kunde inte summera dictionary-v√§rde: {value}, fel: {e}, anv√§nder 0")
            return default
    else:
        return default

def save_results(data, filename):
    """Spara resultaten till en CSV-fil"""
    try:
        # Sortera resultaten efter r√§ckvidd (h√∂gst f√∂rst)
        sorted_data = sorted(data, key=lambda x: safe_int_value(x.get("Reach", 0)), reverse=True)
        
        # Definiera f√§ltnamn baserat p√• tillg√§ngliga nycklar i f√∂rsta raden
        fieldnames = ["Page", "Page ID", "Reach"]
        
        # L√§gg till interaktionsf√§lt om de finns
        if sorted_data and len(sorted_data) > 0:
            if "Engaged Users" in sorted_data[0]:
                fieldnames.append("Engaged Users")
            if "Engagements" in sorted_data[0]:
                fieldnames.append("Engagements")
            if "Reactions" in sorted_data[0]:
                fieldnames.append("Reactions")
            # L√§gg till Status och Comment om de finns
            if "Status" in sorted_data[0]:
                fieldnames.append("Status")
            if "Comment" in sorted_data[0]:
                fieldnames.append("Comment")
        
        with open(filename, mode="w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(sorted_data)
            
        logger.info(f"‚úÖ Sparade data till {filename}")
        return True
    except Exception as e:
        logger.error(f"‚ùå Kunde inte spara data: {e}")
        return False

def get_existing_reports():
    """Scanna katalogen efter befintliga Facebook-r√§ckviddsrapporter och returnera en upps√§ttning av bearbetade m√•nader (YYYY-MM)"""
    existing_reports = set()
    for filename in glob.glob("FB_*.csv"):
        try:
            # Extrahera √•r och m√•nad fr√•n filnamnet (FB_YYYY_MM.csv)
            parts = filename.replace(".csv", "").split("_")
            if len(parts) == 3 and parts[0] == "FB":
                year = parts[1]
                month = parts[2]
                if year.isdigit() and month.isdigit() and len(year) == 4 and len(month) == 2:
                    existing_reports.add(f"{year}-{month}")
                    logger.debug(f"Hittade befintlig rapport f√∂r {year}-{month}: {filename}")
        except Exception as e:
            logger.warning(f"Kunde inte tolka filnamn {filename}: {e}")
    return existing_reports

def get_missing_months(existing_reports, start_year_month):
    """Best√§m vilka m√•nader som beh√∂ver bearbetas"""
    missing_months = []
    
    # Tolka start√•r och m√•nad
    start_year, start_month = map(int, start_year_month.split("-"))
    
    # H√§mta aktuellt √•r och m√•nad
    now = datetime.now()
    current_year = now.year
    current_month = now.month
    
    # Generera alla m√•nader fr√•n startdatum till sista avslutade m√•nad
    year = start_year
    month = start_month
    
    while (year < current_year) or (year == current_year and month < current_month):
        month_str = f"{year}-{month:02d}"
        if month_str not in existing_reports:
            missing_months.append((year, month))
        
        # G√• till n√§sta m√•nad
        month += 1
        if month > 12:
            month = 1
            year += 1
    
    return missing_months

def analyze_page_presence(previous_month, current_month):
    """
    J√§mf√∂r sidor mellan tv√• m√•nader och identifierar nya och bortfallna sidor.
    
    Args:
        previous_month: S√∂kv√§g till CSV-fil f√∂r f√∂reg√•ende m√•nad eller pandas DataFrame
        current_month: S√∂kv√§g till CSV-fil f√∂r aktuell m√•nad eller pandas DataFrame
        
    Returns:
        DataFrame med: Page ID, Page, Status (NY, BORTFALLEN, OF√ñR√ÑNDRAD), M√•nad
    """
    logger.info(f"Analyserar sidn√§rvaro mellan {previous_month} och {current_month}")
    
    # Konvertera till DataFrame om str√§ngar ges
    prev_df = pd.read_csv(previous_month) if isinstance(previous_month, str) else previous_month
    curr_df = pd.read_csv(current_month) if isinstance(current_month, str) else current_month
    
    # Extrahera √•r och m√•nad fr√•n filnamn om det √§r en str√§ng
    if isinstance(current_month, str):
        parts = current_month.replace(".csv", "").split("_")
        if len(parts) >= 3:
            month_str = f"{parts[1]}-{parts[2]}"
        else:
            month_str = "Ok√§nd"
    else:
        month_str = "Aktuell"
    
    # Hitta nya sidor (i current_month men inte i previous_month)
    prev_page_ids = set(prev_df["Page ID"].astype(str))
    curr_page_ids = set(curr_df["Page ID"].astype(str))
    
    new_page_ids = curr_page_ids - prev_page_ids
    missing_page_ids = prev_page_ids - curr_page_ids
    unchanged_page_ids = prev_page_ids.intersection(curr_page_ids)
    
    # Skapa en lista med alla sidor och deras status
    results = []
    
    # L√§gg till nya sidor
    for page_id in new_page_ids:
        page_info = curr_df[curr_df["Page ID"].astype(str) == page_id].iloc[0]
        results.append({
            "Page ID": page_id,
            "Page": page_info["Page"],
            "Status": "NY",
            "M√•nad": month_str,
            "Kommentar": "Inte med i f√∂reg√•ende m√•nad"
        })
    
    # L√§gg till bortfallna sidor
    for page_id in missing_page_ids:
        page_info = prev_df[prev_df["Page ID"].astype(str) == page_id].iloc[0]
        results.append({
            "Page ID": page_id,
            "Page": page_info["Page"],
            "Status": "BORTFALLEN",
            "M√•nad": month_str,
            "Kommentar": "Fanns i f√∂reg√•ende m√•nad"
        })
    
    # L√§gg till of√∂r√§ndrade sidor (valfritt, kan bli m√•nga)
    # for page_id in unchanged_page_ids:
    #     page_info = curr_df[curr_df["Page ID"].astype(str) == page_id].iloc[0]
    #     results.append({
    #         "Page ID": page_id,
    #         "Page": page_info["Page"],
    #         "Status": "OF√ñR√ÑNDRAD",
    #         "M√•nad": month_str,
    #         "Kommentar": ""
    #     })
    
    # L√§gg till statusuppdateringar f√∂r nuvarande m√•nad
    for _, row in curr_df.iterrows():
        page_id = str(row["Page ID"])
        if "Status" in row and row["Status"] != "OK" and row["Status"] != "SKIPPED":
            results.append({
                "Page ID": page_id,
                "Page": row["Page"],
                "Status": row["Status"],
                "M√•nad": month_str,
                "Kommentar": row.get("Comment", "")
            })
    
    # Konvertera till DataFrame och returnera
    result_df = pd.DataFrame(results)
    
    logger.info(f"Analys klar: {len(new_page_ids)} nya sidor, {len(missing_page_ids)} bortfallna sidor")
    
    return result_df

def save_status_report(status_df, year, month):
    """Sparar en statusrapport f√∂r en specifik m√•nad"""
    filename = f"FB_STATUS_{year}_{month:02d}.csv"
    
    try:
        status_df.to_csv(filename, index=False, encoding="utf-8")
        logger.info(f"‚úÖ Sparade statusrapport till {filename}")
        return True
    except Exception as e:
        logger.error(f"‚ùå Kunde inte spara statusrapport: {e}")
        return False

def process_month(year, month, cache, page_list=None, update_all=False, generate_status=True):
    """Bearbeta data f√∂r en specifik m√•nad"""
    # S√§tt datumintervall f√∂r m√•naden
    start_date = f"{year}-{month:02d}-01"
    
    # Ber√§kna slutdatum (sista dagen i m√•naden)
    last_day = monthrange(year, month)[1]
    end_date = f"{year}-{month:02d}-{last_day}"
    
    # S√§tt utdatafilnamn
    output_file = f"FB_{year}_{month:02d}.csv"
    
    logger.info(f"Bearbetar m√•nad: {year}-{month:02d} (fr√•n {start_date} till {end_date})")
    
    # H√§mta sidlista om den inte redan h√§mtats
    if not page_list:
        page_list = get_page_ids_with_access(ACCESS_TOKEN)
    
    if not page_list:
        logger.error("‚ùå Inga sidor hittades. Avbryter.")
        return False
    
    # Kontrollera om det finns befintlig data f√∂r denna m√•nad
    existing_data = {}
    if os.path.exists(output_file) and not update_all:
        existing_data = read_existing_csv(output_file)
        logger.info(f"Hittade {len(existing_data)} befintliga sidor i fil {output_file}")
    
    # Bearbeta data f√∂r denna m√•nad, hoppa √∂ver sidor som redan finns om inte update_all=True
    all_data, ok, fail, skipped = process_in_batches(page_list, cache, start_date, end_date, 
                                                  existing_data=None if update_all else existing_data)
    
    # Spara resultaten
    if all_data:
        save_results(all_data, output_file)
        
        # Visa total r√§ckvidd och interaktioner f√∂r alla sidor med s√§ker summering
        try:
            # Anv√§nd safe_int_value f√∂r att f√∂rhindra typfel vid summering
            total_reach = sum(safe_int_value(item.get("Reach", 0)) for item in all_data)
            
            # Ber√§kna totaler f√∂r interaktioner om tillg√§ngligt
            has_engaged = any("Engaged Users" in item for item in all_data)
            has_engagements = any("Engagements" in item for item in all_data)
            has_reactions = any("Reactions" in item for item in all_data)
            
            if has_engaged:
                total_engaged = sum(safe_int_value(item.get("Engaged Users", 0)) for item in all_data)
            else:
                total_engaged = 0
                
            if has_engagements:
                total_engagements = sum(safe_int_value(item.get("Engagements", 0)) for item in all_data)
            else:
                total_engagements = 0
                
            if has_reactions:
                total_reactions = sum(safe_int_value(item.get("Reactions", 0)) for item in all_data)
            else:
                total_reactions = 0
            
            logger.info(f"üìà Summering f√∂r {year}-{month:02d}:")
            logger.info(f"  - Total r√§ckvidd: {total_reach:,}")
            
            if has_engaged:
                logger.info(f"  - Engagerade anv√§ndare: {total_engaged:,}")
            if has_engagements:
                logger.info(f"  - Totala interaktioner: {total_engagements:,}")
            if has_reactions:
                logger.info(f"  - Reaktioner: {total_reactions:,}")
            
            if skipped > 0:
                logger.info(f"üìà {skipped} sidor fanns redan i CSV-filen och hoppades √∂ver")
                
            # Statusrapport om statuskolumn finns
            status_counts = {}
            for item in all_data:
                if "Status" in item:
                    status = item["Status"]
                    status_counts[status] = status_counts.get(status, 0) + 1
            
            if status_counts:
                logger.info(f"üìã Status√∂versikt:")
                for status, count in status_counts.items():
                    logger.info(f"  - {status}: {count} sidor")
            
            # Generera statusrapport om f√∂reg√•ende m√•nad finns
            if generate_status:
                previous_month = f"{year}-{month-1:02d}" if month > 1 else f"{year-1}-12"
                previous_file = f"FB_{previous_month.split('-')[0]}_{previous_month.split('-')[1]}.csv"
                
                if os.path.exists(previous_file):
                    logger.info(f"Genererar statusrapport genom att j√§mf√∂ra med {previous_file}")
                    try:
                        status_df = analyze_page_presence(previous_file, output_file)
                        save_status_report(status_df, year, month)
                    except Exception as e:
                        logger.error(f"Kunde inte generera statusrapport: {e}")
        
        except Exception as e:
            logger.error(f"Fel vid ber√§kning av summor: {e}")
        
        return True
    else:
        logger.warning(f"‚ö†Ô∏è Inga data att spara f√∂r {year}-{month:02d}")
        return False

def main():
    """Huvudfunktion f√∂r att k√∂ra hela processen"""
    # Parsa kommandoradsargument
    parser = argparse.ArgumentParser(description="Generera Facebook-r√§ckviddsrapport f√∂r alla sidor och m√•nader")
    parser.add_argument("--start", help="Start√•r-m√•nad (YYYY-MM)")
    parser.add_argument("--month", help="K√∂r endast f√∂r angiven m√•nad (YYYY-MM)")
    parser.add_argument("--update-all", action="store_true", help="Uppdatera alla sidor √§ven om de redan finns i CSV-filen")
    parser.add_argument("--check-new", action="store_true", help="Kontrollera efter nya sidor i alla befintliga m√•nader")
    parser.add_argument("--status", help="Generera endast statusrapport f√∂r angiven m√•nad (YYYY-MM)")
    parser.add_argument("--debug", action="store_true", help="Aktivera debug-loggning")
    args = parser.parse_args()
    
    # S√§tt debug-l√§ge om beg√§rt
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.debug("Debug-l√§ge aktiverat")
    
    # Anv√§nd argument om de finns
    start_year_month = args.start or INITIAL_START_YEAR_MONTH
    
    logger.info(f"üìä Facebook Reach & Interactions Report Generator ‚Äì v2.0")
    logger.info(f"Startdatum: {start_year_month}")
    logger.info("-------------------------------------------------------------------")
    
    # Kontrollera token och varna om den snart g√•r ut
    check_token_expiry()
    
    # Validera token
    if not validate_token(ACCESS_TOKEN):
        logger.error("‚ùå Token kunde inte valideras. Avbryter.")
        return
    
    # Ladda cache f√∂r sidnamn
    cache = load_page_cache()
    
    # Om --status anv√§nds, generera endast statusrapport
    if args.status:
        try:
            year, month = map(int, args.status.split("-"))
            current_file = f"FB_{year}_{month:02d}.csv"
            
            if not os.path.exists(current_file):
                logger.error(f"‚ùå Fil {current_file} hittades inte. Kan inte generera statusrapport.")
                return
                
            # Best√§m f√∂reg√•ende m√•nad
            if month > 1:
                prev_month = month - 1
                prev_year = year
            else:
                prev_month = 12
                prev_year = year - 1
                
            prev_file = f"FB_{prev_year}_{prev_month:02d}.csv"
            
            if not os.path.exists(prev_file):
                logger.error(f"‚ùå Fil {prev_file} hittades inte. Kan inte j√§mf√∂ra med f√∂reg√•ende m√•nad.")
                return
                
            logger.info(f"Genererar statusrapport f√∂r {year}-{month:02d}")
            status_df = analyze_page_presence(prev_file, current_file)
            save_status_report(status_df, year, month)
            return
        except Exception as e:
            logger.error(f"‚ùå Fel vid generering av statusrapport: {e}")
            return
    
    # H√§mta alla tillg√§ngliga sidor (en g√•ng f√∂r alla m√•nader)
    page_list = get_page_ids_with_access(ACCESS_TOKEN)
    
    if not page_list:
        logger.error("‚ùå Inga sidor hittades. Avbryter.")
        return
    
    # Om check-new-argument, kontrollera alla befintliga m√•nader efter nya sidor
    if args.check_new:
        logger.info("Kontrollerar efter nya sidor i alla befintliga m√•nader...")
        existing_reports = get_existing_reports()
        
        for report in sorted(existing_reports):
            year, month = map(int, report.split("-"))
            logger.info(f"Kontrollerar {year}-{month:02d} efter nya sidor...")
            process_month(year, month, cache, page_list, update_all=args.update_all, generate_status=True)
            
        logger.info("‚úÖ Kontroll efter nya sidor slutf√∂rd")
        save_page_cache(cache)
        return
    
    # Om specifik m√•nad angivits, k√∂r endast den
    if args.month:
        try:
            year, month = map(int, args.month.split("-"))
            logger.info(f"K√∂r endast f√∂r specifik m√•nad: {year}-{month:02d}")
            process_month(year, month, cache, page_list, update_all=args.update_all, generate_status=True)
            save_page_cache(cache)
            return
        except ValueError:
            logger.error(f"Ogiltigt m√•nadsformat: {args.month}. Anv√§nd YYYY-MM.")
            return
    
    # H√§mta befintliga rapporter
    existing_reports = get_existing_reports()
    logger.info(f"Hittade {len(existing_reports)} befintliga rapporter: {', '.join(sorted(existing_reports)) if existing_reports else 'Inga'}")
    
    # F√• saknade m√•nader
    missing_months = get_missing_months(existing_reports, start_year_month)
    
    if not missing_months:
        logger.info("‚úÖ Alla m√•nader √§r redan bearbetade. Inget att g√∂ra.")
        logger.info("Om du vill kontrollera efter nya sidor i befintliga rapporter, anv√§nd --check-new")
        return
    
    logger.info(f"Beh√∂ver bearbeta {len(missing_months)} saknade m√•nader: {', '.join([f'{y}-{m:02d}' for y, m in missing_months])}")
    
    # Bearbeta varje saknad m√•nad
    for year, month in missing_months:
        logger.info(f"‚è≥ Bearbetar data f√∂r {year}-{month:02d}...")
        
        # Bearbeta denna m√•nad
        success = process_month(year, month, cache, page_list, update_all=args.update_all, generate_status=True)
        
        # Spara cache efter varje m√•nad
        save_page_cache(cache)
        
        if not success:
            logger.warning(f"‚ö†Ô∏è Kunde inte slutf√∂ra bearbetningen f√∂r {year}-{month:02d}")
        else:
            logger.info(f"‚úÖ Slutf√∂rde bearbetningen f√∂r {year}-{month:02d}")
        
        # Pausa f√∂r att respektera API-begr√§nsningar om det finns fler m√•nader att bearbeta
        if missing_months.index((year, month)) < len(missing_months) - 1:
            logger.info(f"Pausar i {MONTH_PAUSE_SECONDS} sekunder f√∂r att respektera API-begr√§nsningar...")
            time.sleep(MONTH_PAUSE_SECONDS)
    
    # Visa statistik om API-anv√§ndning
    elapsed_time = time.time() - start_time
    logger.info(f"‚è±Ô∏è Total k√∂rtid: {elapsed_time:.1f} sekunder")
    logger.info(f"üåê API-anrop: {api_call_count} ({api_call_count/elapsed_time*3600:.1f}/timme)")
    logger.info(f"‚úÖ Klar! Bearbetade {len(missing_months)} m√•nader")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("Avbruten av anv√§ndare. Sparar mellanlagrat arbete...")
        # H√§r kunde vi implementera att spara framsteg
        sys.exit(1)
    except Exception as e:
        logger.critical(f"Ov√§ntat fel: {e}")
        import traceback
        logger.critical(traceback.format_exc())
        sys.exit(1)
